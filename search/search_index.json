{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ScraperNHL Documentation","text":"<p>NHL data scraping package with Expected Goals (xG) model.</p>"},{"location":"#overview","title":"Overview","text":"<p>ScraperNHL is a Python package designed for scraping and analyzing NHL data. This documentation will guide you through the installation, usage, and features of the package.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Fast NHL data scraping using <code>selectolax</code></li> <li>Pre-trained XGBoost Expected Goals model</li> <li>Command-line interface (CLI) for quick data exports</li> <li>Support for play-by-play data with xG calculations</li> <li>Efficient data processing with <code>polars</code></li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Getting Started</li> <li>API Reference</li> <li>CLI Examples - Command-line usage</li> <li>Python Examples - Python API usage</li> <li>About the Project</li> <li>GitHub Repository</li> <li>PyPI Package</li> <li>Announcements - Latest news and updates</li> </ul>"},{"location":"MODULARIZATION/","title":"ScraperNHL - Modularization Guide","text":""},{"location":"MODULARIZATION/#status-phase-1-complete-v014","title":"\u2705 Status: Phase 1 Complete (v0.1.4)","text":"<p>The scraper codebase has been successfully modularized while maintaining 100% backward compatibility. The original monolithic <code>scraper.py</code> (~5000 lines) has been split into focused, single-responsibility modules.</p> <p>All tests passing \u2705 | Original code backed up \u2705 | Zero breaking changes \u2705 | CLI added \u2705</p>"},{"location":"MODULARIZATION/#current-structure","title":"Current Structure","text":"<pre><code>scrapernhl/\n\u251c\u2500\u2500 __init__.py                 # Public API exports\n\u251c\u2500\u2500 __main__.py                 # CLI entry point\n\u251c\u2500\u2500 cli.py                      # Command-line interface (click-based)\n\u251c\u2500\u2500 config.py                   # Constants, headers, API endpoints\n\u251c\u2500\u2500 scraper.py                  # Backward-compatible re-exports with lazy loading\n\u251c\u2500\u2500 scraper_legacy.py           # BACKUP: Original monolithic file (for safety)\n\u2502\n\u251c\u2500\u2500 core/                       # Core utilities \u2705 COMPLETED\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 http.py                 # fetch_json, fetch_html, async variants\n\u2502   \u2514\u2500\u2500 utils.py                # time_str_to_seconds, json_normalize, etc.\n\u2502\n\u251c\u2500\u2500 scrapers/                   # Data fetching modules \u2705 COMPLETED\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 teams.py                # getTeamsData, scrapeTeams\n\u2502   \u251c\u2500\u2500 schedule.py             # getScheduleData, scrapeSchedule\n\u2502   \u251c\u2500\u2500 standings.py            # getStandingsData, scrapeStandings\n\u2502   \u251c\u2500\u2500 roster.py               # getRosterData, scrapeRoster\n\u2502   \u251c\u2500\u2500 stats.py                # getTeamStatsData, scrapeTeamStats\n\u2502   \u251c\u2500\u2500 draft.py                # Draft-related scrapers (all variants)\n\u2502   \u2514\u2500\u2500 games.py                # getGameData, scrapePlays, goal replays\n\u2502\n\u2514\u2500\u2500 models/                     # ML models \u2705 PRESENT\n    \u251c\u2500\u2500 xgboost_xG_model1.json  # XGBoost Expected Goals model\n    \u2514\u2500\u2500 xgboost_xG_features1.pkl # Feature definitions for xG model\n</code></pre>"},{"location":"MODULARIZATION/#future-modules-phase-2-not-yet-created","title":"Future Modules (Phase 2 - Not Yet Created)","text":"<pre><code>scrapernhl/\n\u251c\u2500\u2500 pbp/                        # Play-by-play processing (PLANNED)\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 parsers.py              # parse_html_pbp, parse_html_shifts\n\u2502   \u251c\u2500\u2500 coordinates.py          # _add_normalized_coordinates\n\u2502   \u2514\u2500\u2500 events.py               # Event-related processing\n\u2502\n\u251c\u2500\u2500 features/                   # Feature engineering (PLANNED)\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 xg.py                   # engineer_xg_features, predict_xg_for_pbp\n\u2502   \u251c\u2500\u2500 on_ice.py               # build_on_ice_long, build_on_ice_wide\n\u2502   \u251c\u2500\u2500 strengths.py            # build_strength_segments, etc.\n\u2502   \u2514\u2500\u2500 shifts.py               # build_shifts_events, etc.\n\u2502\n\u2514\u2500\u2500 analysis/                   # Analytics functions (PLANNED)\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 toi.py                  # toi_by_strength, shared_toi_*\n    \u251c\u2500\u2500 combos.py               # combos_teammates_by_strength, etc.\n    \u251c\u2500\u2500 stats.py                # on_ice_stats_by_player_strength, etc.\n    \u2514\u2500\u2500 aggregates.py           # team_strength_aggregates, etc.\n</code></pre>"},{"location":"MODULARIZATION/#usage","title":"Usage","text":""},{"location":"MODULARIZATION/#new-modular-style-recommended","title":"New Modular Style (Recommended)","text":"<p>Import directly from submodules for faster loading:</p> <pre><code>from scrapernhl.scrapers.teams import scrapeTeams\nfrom scrapernhl.scrapers.schedule import scrapeSchedule\nfrom scrapernhl.scrapers.standings import scrapeStandings\n\n# Fast imports, no heavy dependencies\nteams = scrapeTeams()\nschedule = scrapeSchedule(\"MTL\", \"20252026\")\nstandings = scrapeStandings(\"2025-01-01\")\n</code></pre>"},{"location":"MODULARIZATION/#legacy-style-still-works","title":"Legacy Style (Still Works)","text":"<p>The old API is fully backward compatible:</p> <pre><code>from scrapernhl import scrapeTeams, scrapeSchedule, scrapeStandings\n\n# Everything works as before\nteams = scrapeTeams()\nschedule = scrapeSchedule(\"MTL\", \"20252026\")\n</code></pre>"},{"location":"MODULARIZATION/#cli-usage","title":"CLI Usage","text":"<p>The package includes a comprehensive command-line interface:</p> <pre><code># View available commands\nscrapernhl --help\n# or\npython -m scrapernhl --help\n\n# Scrape teams\nscrapernhl teams --output teams.csv\n\n# Scrape schedule\nscrapernhl schedule MTL 20252026 --format json\n\n# Scrape standings\nscrapernhl standings --output standings.parquet --format parquet\n\n# Scrape game with xG\nscrapernhl game 2024020001 --with-xg\n\n# See all CLI examples in the documentation\n</code></pre>"},{"location":"MODULARIZATION/#testing","title":"Testing","text":"<p>Quick tests from command line:</p> <pre><code># Test import\npython -c \"from scrapernhl.scrapers.teams import scrapeTeams; print('\u2713 Works')\"\n\n# Test scraping\npython -c \"from scrapernhl import scrapeTeams; print(f'{len(scrapeTeams())} teams')\"\n\n# Run test suite (if available)\npytest tests/\n\n# Run interactive demo (if available)\npython demo_modular.py\n</code></pre>"},{"location":"MODULARIZATION/#benefits","title":"Benefits","text":"<ol> <li>Faster imports: Basic scrapers load in ~100ms (vs 2-3s previously)</li> <li>Better organization: Each module has a single responsibility</li> <li>Easier testing: Can test individual modules in isolation</li> <li>Improved docs: Smaller files are easier to document and understand</li> <li>Safer refactoring: Original code backed up in <code>scraper_legacy.py</code></li> <li>Clearer dependencies: Know exactly what each module requires</li> <li>CLI integration: Command-line interface for quick data exports</li> <li>Lazy loading: Heavy dependencies (xgboost) only load when needed</li> </ol>"},{"location":"MODULARIZATION/#available-modules","title":"Available Modules","text":""},{"location":"MODULARIZATION/#scrapers-scrapernhlscrapers","title":"Scrapers (<code>scrapernhl.scrapers</code>)","text":"Module Functions Description <code>teams</code> <code>scrapeTeams()</code> NHL team data <code>schedule</code> <code>scrapeSchedule(team, season)</code> Team schedule <code>standings</code> <code>scrapeStandings(date)</code> League standings <code>roster</code> <code>scrapeRoster(team, season)</code> Team rosters <code>stats</code> <code>scrapeTeamStats(team, season)</code> Player statistics <code>draft</code> <code>scrapeDraftData(year)</code> Draft picks <code>games</code> <code>scrapePlays(game_id)</code> Play-by-play data"},{"location":"MODULARIZATION/#core-utilities-scrapernhlcore","title":"Core Utilities (<code>scrapernhl.core</code>)","text":"Module Functions Description <code>http</code> <code>fetch_json()</code>, <code>fetch_html()</code> HTTP fetching with retry <code>utils</code> <code>json_normalize()</code>, <code>time_str_to_seconds()</code> Helper functions"},{"location":"MODULARIZATION/#migration-status","title":"Migration Status","text":""},{"location":"MODULARIZATION/#completed-phase-1","title":"\u2705 Completed (Phase 1)","text":"<ul> <li>Core utilities (<code>scrapernhl.core.http</code>, <code>scrapernhl.core.utils</code>)</li> <li>Configuration module (<code>scrapernhl.config</code>)</li> <li>All basic scrapers (<code>scrapernhl.scrapers.*</code>)</li> <li>teams, schedule, standings, roster, stats, draft, games</li> <li>Command-line interface (<code>scrapernhl.cli</code>)</li> <li>Backward compatibility layer (<code>scrapernhl.scraper</code>)</li> <li>Lazy loading for legacy/advanced functions</li> <li>XGBoost models for Expected Goals</li> <li>Package distribution (PyPI)</li> <li>Comprehensive documentation website</li> </ul>"},{"location":"MODULARIZATION/#to-be-created-phase-2","title":"\ud83d\udd04 To Be Created (Phase 2)","text":"<ul> <li>PBP parsing module (<code>scrapernhl.pbp</code>)</li> <li>Feature engineering modules (<code>scrapernhl.features</code>)</li> <li>Analysis modules (<code>scrapernhl.analysis</code>)</li> <li>Pipeline orchestration</li> <li>Additional unit tests for edge cases</li> <li>Performance benchmarking</li> </ul>"},{"location":"MODULARIZATION/#testing_1","title":"Testing","text":"<pre><code># Run test suite with pytest\npytest tests/ -v\n\n# Run specific test file\npytest tests/test_modular.py -v\n\n# Quick inline tests\npython -c \"from scrapernhl import scrapeTeams; print(f'{len(scrapeTeams())} teams')\"\n\n# Test modular imports\npython -c \"from scrapernhl.scrapers.teams import scrapeTeams; print('\u2713 Modular imports work')\"\n\n# Test CLI\nscrapernhl --help\nscrapernhl teams --output test_teams.csv\n</code></pre>"},{"location":"MODULARIZATION/#safety","title":"Safety","text":"<ul> <li>Original code preserved: <code>scraper_legacy.py</code> contains the full original implementation</li> <li>Lazy loading: Heavy dependencies only load when advanced features are used</li> <li>100% backward compatible: Existing code continues to work without changes</li> </ul>"},{"location":"MODULARIZATION/#next-steps-phase-2","title":"Next Steps (Phase 2)","text":"<ol> <li>Modularize PBP parsing \u2192 Create <code>scrapernhl/pbp/</code> module</li> <li><code>parsers.py</code> - HTML parsing functions</li> <li><code>coordinates.py</code> - Coordinate normalization</li> <li> <p><code>events.py</code> - Event processing</p> </li> <li> <p>Extract feature engineering \u2192 Create <code>scrapernhl/features/</code> module</p> </li> <li><code>xg.py</code> - Expected goals features and predictions</li> <li><code>on_ice.py</code> - On-ice player tracking</li> <li><code>strengths.py</code> - Strength state analysis</li> <li> <p><code>shifts.py</code> - Shift processing</p> </li> <li> <p>Organize analysis functions \u2192 Create <code>scrapernhl/analysis/</code> module</p> </li> <li><code>toi.py</code> - Time on ice calculations</li> <li><code>combos.py</code> - Line combination analysis</li> <li><code>stats.py</code> - Player and team statistics</li> <li> <p><code>aggregates.py</code> - Team-level aggregations</p> </li> <li> <p>Testing &amp; Quality</p> </li> <li>Add comprehensive unit tests for each module</li> <li>Add integration tests for full workflows</li> <li>Performance benchmarking and optimization</li> <li> <p>Code coverage reporting</p> </li> <li> <p>Documentation</p> </li> <li>Update API reference with all new modules</li> <li>Add more usage examples</li> <li>Create migration guide for legacy code</li> </ol> <p>Related Files: - Main guide: <code>MODULARIZATION.md</code> (this file) - Test suite: <code>tests/test_modular.py</code> - CLI implementation: <code>scrapernhl/cli.py</code> - Package config: <code>pyproject.toml</code> - Documentation: <code>docs/</code> directory</p>"},{"location":"about/","title":"About","text":"<p>ScraperNHL is an open-source project dedicated to scraping, processing, and analyzing NHL data. It provides tools to collect raw league data, transform it into usable datasets, and support modeling and analytical workflows for hockey research.</p> <p>This is roughly the tenth NHL scraper I\u2019ve built since I started coding in 2017. Over time, I wanted something more efficient, more maintainable, and designed as a long-term project rather than a collection of one-off scripts. While this could have remained a personal tool, I chose to make it public. The hockey analytics community has helped me immensely over the years, and this project is my way of giving back.</p> <p>ScraperNHL is a community project. Contributions of all kinds are welcome\u2014bug fixes, new features, documentation improvements, or ideas. This is not my project; it is our project. Fork it, use it, improve it, and make it your own. If you have questions, feedback, or suggestions, feel free to reach out.</p>"},{"location":"about/#author","title":"Author","text":"Name Role X / Twitter Bluesky Website Max Tixador Hockey Analytics Enthusiast @woumaxx @HabsBrain.com HabsBrain.com <p>Hello, I\u2019m Max Tixador, a hockey analytics enthusiast and data analyst with a strong interest in using data to better understand and appreciate the game of hockey.</p> <p>My path into hockey analytics is unconventional. I come from countries where hockey is either nonexistent or a tertiary sport, no one in my family follows sports closely, and I never played competitive hockey. As a kid, I memorized sports statistics by reading fantasy hockey magazines, which sparked my interest in hockey journalism. In my teens, I discovered hockey analytics and became fascinated by how data could explain the game beyond traditional statistics.</p> <p>I later explored hockey graphic design and content creation, teaching myself visual design and social media growth. I originally learned to code to automate content creation, but quickly realized that programming could also unlock far more powerful ways to scrape and analyze hockey data.</p> <p>A turning point came when I met Mikhail Nahabedian, then with the McGill Redbirds and now Director of Hockey Analytics for La Victoire de Montr\u00e9al. Through his free online hockey analytics seminar series, he mentored me and introduced me to structured analytical thinking in hockey. Under his guidance, I spent countless nights learning to code, scrape data, build models, and create visualizations. Despite having no formal background in data science or programming, he helped me land my first professional opportunities in hockey analytics.</p> <p>Since then, I\u2019ve continued to grow independently through self-study and practice. My work includes building an Expected Goals model, developing a Regularized Adjusted Plus-Minus (RAPM) model, contributing to The Draft Digest (a data-driven draft prospect evaluation project), submitting work to hockey analytics competitions, and helping introduce several aspiring analysts to the field.</p> <p>I strongly believe in end-to-end understanding\u2014from data collection, to modeling, to visualization. I also believe growth does not happen in isolation. Collaboration, knowledge sharing, and community feedback are essential, and I am always open to discussion, critique, and new ideas.</p> <p>Outside of hockey analytics, I am learning Mandarin Chinese (currently at a beginner level) and play soccer.</p> <p>I operate under a simple belief: I will never know enough, and there will always be more to learn. That mindset drives both this project and my broader work.</p> <p>I am open to collaborations, new projects, and opportunities related to hockey analytics. I am also interested in eventually formalizing my self-taught background through a degree or professional certification in data science or a related field.</p> <p>Most importantly, I want to thank the hockey analytics community for its support and openness. This project exists because of that community, and I look forward to building more tools for it.</p> <p>Contact: maxtixador@gmail.com</p>"},{"location":"api/","title":"API Reference","text":"<p>ScraperNHL API documentation. The scraper is split into modules for easier use and faster imports.</p>"},{"location":"api/#quick-start","title":"Quick Start","text":""},{"location":"api/#python-api","title":"Python API","text":""},{"location":"api/#modular-imports-recommended","title":"Modular Imports (Recommended)","text":"<pre><code>from scrapernhl.scrapers.teams import scrapeTeams\nfrom scrapernhl.scrapers.schedule import scrapeSchedule\n\nteams = scrapeTeams()\nschedule = scrapeSchedule(\"MTL\", \"20252026\")\n</code></pre>"},{"location":"api/#legacy-imports-still-supported","title":"Legacy Imports (Still Supported)","text":"<pre><code>from scrapernhl import scrapeTeams, scrapeSchedule\n\nteams = scrapeTeams()\nschedule = scrapeSchedule(\"MTL\", \"20252026\")\n</code></pre>"},{"location":"api/#command-line-interface","title":"Command-Line Interface","text":"<p>All scraping functions are also available via CLI for quick data exports:</p> <pre><code># View available commands\npython scrapernhl/cli.py --help\n\n# Scrape teams\npython scrapernhl/cli.py teams --output teams.csv\n\n# Scrape schedule\npython scrapernhl/cli.py schedule MTL 20252026 --format json\n\n# Scrape with different output formats\npython scrapernhl/cli.py standings --format parquet\n</code></pre> <p>See the CLI Examples page for comprehensive CLI usage.</p>"},{"location":"api/#important-note-about-game-strengths","title":"IMPORTANT NOTE ABOUT GAME STRENGTHS","text":"<p>In order to differenciate empty-net situtations to non-empty net situations, there the scraper incorporates a unique format. It uses the star symbol to indentify the team with an empty net.</p> <p>If you ever see <code>5v6*</code>, it means that the player or team is facing a team that has 6 skaters and the net empty.  If you see <code>5*v4</code>, it means that the player or team has 5 skaters and the net empty while facing a team with 4 skaters with a goalie. </p>"},{"location":"api/#scrapers-module-scrapernhlscrapers","title":"Scrapers Module (<code>scrapernhl.scrapers</code>)","text":""},{"location":"api/#teams-scrapernhlscrapersteams","title":"Teams (<code>scrapernhl.scrapers.teams</code>)","text":""},{"location":"api/#getteamsdatasource-str-default-listdict","title":"<code>getTeamsData(source: str = \"default\") -&gt; List[Dict]</code>","text":"<p>Scrapes raw NHL team data from various public endpoints.</p> <p>Parameters: - <code>source</code> (str): Data source to use. Options: <code>\"default\"</code>, <code>\"calendar\"</code>, <code>\"records\"</code></p> <p>Returns: - <code>List[Dict]</code>: Raw team data with metadata</p> <p>Example: <pre><code>from scrapernhl.scrapers.teams import getTeamsData\n\nraw_teams = getTeamsData(source=\"default\")\n</code></pre></p>"},{"location":"api/#scrapeteamssource-str-default-output_format-str-pandas-dataframe","title":"<code>scrapeTeams(source: str = \"default\", output_format: str = \"pandas\") -&gt; DataFrame</code>","text":"<p>Scrapes NHL team data and returns as DataFrame.</p> <p>Parameters: - <code>source</code> (str): Data source. Options: <code>\"default\"</code>, <code>\"calendar\"</code>, <code>\"records\"</code> - <code>output_format</code> (str): Output format. Options: <code>\"pandas\"</code>, <code>\"polars\"</code></p> <p>Returns: - <code>pd.DataFrame</code> or <code>pl.DataFrame</code>: Team data with metadata</p> <p>Example: <pre><code>from scrapernhl.scrapers.teams import scrapeTeams\n\n# Get as pandas DataFrame\nteams = scrapeTeams()\n\n# Get as polars DataFrame\nteams_pl = scrapeTeams(output_format=\"polars\")\n</code></pre></p>"},{"location":"api/#schedule-scrapernhlscrapersschedule","title":"Schedule (<code>scrapernhl.scrapers.schedule</code>)","text":""},{"location":"api/#getscheduledatateam-str-mtl-season-unionstr-int-20252026-listdict","title":"<code>getScheduleData(team: str = \"MTL\", season: Union[str, int] = \"20252026\") -&gt; List[Dict]</code>","text":"<p>Scrapes raw NHL schedule data for a team and season.</p> <p>Parameters: - <code>team</code> (str): Team abbreviation (e.g., \"MTL\", \"TOR\", \"BOS\") - <code>season</code> (str | int): Season ID (e.g., \"20252026\" or 20252026)</p> <p>Returns: - <code>List[Dict]</code>: Raw schedule records</p> <p>Example: <pre><code>from scrapernhl.scrapers.schedule import getScheduleData\n\nraw_schedule = getScheduleData(\"MTL\", \"20252026\")\n</code></pre></p>"},{"location":"api/#scrapescheduleteam-str-mtl-season-unionstr-int-20252026-output_format-str-pandas-dataframe","title":"<code>scrapeSchedule(team: str = \"MTL\", season: Union[str, int] = \"20252026\", output_format: str = \"pandas\") -&gt; DataFrame</code>","text":"<p>Scrapes NHL schedule data for a team and season.</p> <p>Parameters: - <code>team</code> (str): Team abbreviation - <code>season</code> (str | int): Season ID - <code>output_format</code> (str): Output format. Options: <code>\"pandas\"</code>, <code>\"polars\"</code></p> <p>Returns: - <code>pd.DataFrame</code> or <code>pl.DataFrame</code>: Schedule data</p> <p>Example: <pre><code>from scrapernhl.scrapers.schedule import scrapeSchedule\n\n# Montreal Canadiens 2025-26 season\nschedule = scrapeSchedule(\"MTL\", \"20252026\")\n</code></pre></p>"},{"location":"api/#standings-scrapernhlscrapersstandings","title":"Standings (<code>scrapernhl.scrapers.standings</code>)","text":""},{"location":"api/#getstandingsdatadate-str-none-listdict","title":"<code>getStandingsData(date: str = None) -&gt; List[Dict]</code>","text":"<p>Scrapes raw NHL standings data for a specific date.</p> <p>Parameters: - <code>date</code> (str, optional): Date in 'YYYY-MM-DD' format. Defaults to previous year's January 1st</p> <p>Returns: - <code>List[Dict]</code>: Raw standings records</p> <p>Example: <pre><code>from scrapernhl.scrapers.standings import getStandingsData\n\nraw_standings = getStandingsData(\"2025-01-01\")\n</code></pre></p>"},{"location":"api/#scrapestandingsdate-str-none-output_format-str-pandas-dataframe","title":"<code>scrapeStandings(date: str = None, output_format: str = \"pandas\") -&gt; DataFrame</code>","text":"<p>Scrapes NHL standings data for a specific date.</p> <p>Parameters: - <code>date</code> (str, optional): Date in 'YYYY-MM-DD' format - <code>output_format</code> (str): Output format. Options: <code>\"pandas\"</code>, <code>\"polars\"</code></p> <p>Returns: - <code>pd.DataFrame</code> or <code>pl.DataFrame</code>: Standings data</p> <p>Example: <pre><code>from scrapernhl.scrapers.standings import scrapeStandings\n\nstandings = scrapeStandings(\"2025-01-01\")\n</code></pre></p>"},{"location":"api/#roster-scrapernhlscrapersroster","title":"Roster (<code>scrapernhl.scrapers.roster</code>)","text":""},{"location":"api/#getrosterdatateam-str-mtl-season-unionstr-int-20242025-listdict","title":"<code>getRosterData(team: str = \"MTL\", season: Union[str, int] = \"20242025\") -&gt; List[Dict]</code>","text":"<p>Scrapes raw NHL roster data for a team and season.</p> <p>Parameters: - <code>team</code> (str): Team abbreviation - <code>season</code> (str | int): Season ID</p> <p>Returns: - <code>List[Dict]</code>: Raw roster records (forwards, defensemen, goalies)</p> <p>Example: <pre><code>from scrapernhl.scrapers.roster import getRosterData\n\nraw_roster = getRosterData(\"MTL\", \"20242025\")\n</code></pre></p>"},{"location":"api/#scraperosterteam-str-mtl-season-unionstr-int-20242025-output_format-str-pandas-dataframe","title":"<code>scrapeRoster(team: str = \"MTL\", season: Union[str, int] = \"20242025\", output_format: str = \"pandas\") -&gt; DataFrame</code>","text":"<p>Scrapes NHL roster data for a team and season.</p> <p>Parameters: - <code>team</code> (str): Team abbreviation - <code>season</code> (str | int): Season ID - <code>output_format</code> (str): Output format</p> <p>Returns: - <code>pd.DataFrame</code> or <code>pl.DataFrame</code>: Roster data</p> <p>Example: <pre><code>from scrapernhl.scrapers.roster import scrapeRoster\n\nroster = scrapeRoster(\"MTL\", \"20252026\")\n</code></pre></p>"},{"location":"api/#stats-scrapernhlscrapersstats","title":"Stats (<code>scrapernhl.scrapers.stats</code>)","text":""},{"location":"api/#getteamstatsdatateam-str-mtl-season-unionstr-int-20252026-session-unionstr-int-2-goalies-bool-false-listdict","title":"<code>getTeamStatsData(team: str = \"MTL\", season: Union[str, int] = \"20252026\", session: Union[str, int] = 2, goalies: bool = False) -&gt; List[Dict]</code>","text":"<p>Scrapes raw NHL team statistics.</p> <p>Parameters: - <code>team</code> (str): Team abbreviation - <code>season</code> (str | int): Season ID - <code>session</code> (str | int): Session type - 1: pre-season, 2: regular season, 3: playoffs - <code>goalies</code> (bool): If True, fetch goalie stats; if False, fetch skater stats</p> <p>Returns: - <code>List[Dict]</code>: Raw statistics records</p> <p>Example: <pre><code>from scrapernhl.scrapers.stats import getTeamStatsData\n\n# Get skater stats\nskater_stats = getTeamStatsData(\"MTL\", \"20252026\", session=2, goalies=False)\n\n# Get goalie stats\ngoalie_stats = getTeamStatsData(\"MTL\", \"20252026\", session=2, goalies=True)\n</code></pre></p>"},{"location":"api/#scrapeteamstatsteam-str-mtl-season-unionstr-int-20252026-session-unionstr-int-2-goalies-bool-false-output_format-str-pandas-dataframe","title":"<code>scrapeTeamStats(team: str = \"MTL\", season: Union[str, int] = \"20252026\", session: Union[str, int] = 2, goalies: bool = False, output_format: str = \"pandas\") -&gt; DataFrame</code>","text":"<p>Scrapes NHL team statistics.</p> <p>Parameters: - <code>team</code> (str): Team abbreviation - <code>season</code> (str | int): Season ID - <code>session</code> (str | int): Session type - <code>goalies</code> (bool): Fetch goalie stats vs skater stats - <code>output_format</code> (str): Output format</p> <p>Returns: - <code>pd.DataFrame</code> or <code>pl.DataFrame</code>: Team statistics</p> <p>Example: <pre><code>from scrapernhl.scrapers.stats import scrapeTeamStats\n\nstats = scrapeTeamStats(\"MTL\", \"20252026\", session=2)\n</code></pre></p>"},{"location":"api/#draft-scrapernhlscrapersdraft","title":"Draft (<code>scrapernhl.scrapers.draft</code>)","text":""},{"location":"api/#getdraftdatadatayear-unionstr-int-2024-round-unionstr-int-all-listdict","title":"<code>getDraftDataData(year: Union[str, int] = \"2024\", round: Union[str, int] = \"all\") -&gt; List[Dict]</code>","text":"<p>Scrapes raw NHL draft data.</p> <p>Parameters: - <code>year</code> (str | int): Draft year (e.g., \"2024\") - <code>round</code> (str | int): Round number or \"all\" for all rounds</p> <p>Returns: - <code>List[Dict]</code>: Raw draft records</p>"},{"location":"api/#scrapedraftdatayear-unionstr-int-2024-round-unionstr-int-all-output_format-str-pandas-dataframe","title":"<code>scrapeDraftData(year: Union[str, int] = \"2024\", round: Union[str, int] = \"all\", output_format: str = \"pandas\") -&gt; DataFrame</code>","text":"<p>Scrapes NHL draft data.</p> <p>Parameters: - <code>year</code> (str | int): Draft year - <code>round</code> (str | int): Round number or \"all\" - <code>output_format</code> (str): Output format</p> <p>Returns: - <code>pd.DataFrame</code> or <code>pl.DataFrame</code>: Draft data</p> <p>Example: <pre><code>from scrapernhl.scrapers.draft import scrapeDraftData\n\n# All rounds\ndraft = scrapeDraftData(\"2024\", \"all\")\n\n# First round only\nfirst_round = scrapeDraftData(\"2024\", 1)\n</code></pre></p>"},{"location":"api/#getrecordsdraftdatayear-unionstr-int-2025-listdict","title":"<code>getRecordsDraftData(year: Union[str, int] = \"2025\") -&gt; List[Dict]</code>","text":"<p>Scrapes draft records from NHL Records API.</p>"},{"location":"api/#scrapedraftrecordsyear-unionstr-int-2025-output_format-str-pandas-dataframe","title":"<code>scrapeDraftRecords(year: Union[str, int] = \"2025\", output_format: str = \"pandas\") -&gt; DataFrame</code>","text":"<p>Scrapes draft records from NHL Records API.</p>"},{"location":"api/#getrecordsteamdrafthistorydatafranchise-unionstr-int-1-listdict","title":"<code>getRecordsTeamDraftHistoryData(franchise: Union[str, int] = 1) -&gt; List[Dict]</code>","text":"<p>Scrapes team draft history for a franchise.</p> <p>Parameters: - <code>franchise</code> (str | int): Franchise ID</p>"},{"location":"api/#scrapeteamdrafthistoryfranchise-unionstr-int-1-output_format-str-pandas-dataframe","title":"<code>scrapeTeamDraftHistory(franchise: Union[str, int] = 1, output_format: str = \"pandas\") -&gt; DataFrame</code>","text":"<p>Scrapes team draft history.</p> <p>Example: <pre><code>from scrapernhl.scrapers.draft import scrapeTeamDraftHistory\n\n# Montreal Canadiens (franchise ID 1)\nmtl_draft_history = scrapeTeamDraftHistory(1)\n</code></pre></p>"},{"location":"api/#games-scrapernhlscrapersgames","title":"Games (<code>scrapernhl.scrapers.games</code>)","text":""},{"location":"api/#getgamedatagame-unionstr-int-addgoalreplaydata-bool-false-dict","title":"<code>getGameData(game: Union[str, int], addGoalReplayData: bool = False) -&gt; Dict</code>","text":"<p>Scrapes NHL play-by-play data for a game.</p> <p>Parameters: - <code>game</code> (str | int): Game ID (e.g., 2024020001) - <code>addGoalReplayData</code> (bool): If True, fetch goal replay data</p> <p>Returns: - <code>Dict</code>: Complete game data with enriched plays</p> <p>Example: <pre><code>from scrapernhl.scrapers.games import getGameData\n\ngame_data = getGameData(2024020001, addGoalReplayData=True)\n</code></pre></p>"},{"location":"api/#scrapeplaysgame-unionstr-int-addgoalreplaydata-bool-false-output_format-str-pandas-dataframe","title":"<code>scrapePlays(game: Union[str, int], addGoalReplayData: bool = False, output_format: str = \"pandas\") -&gt; DataFrame</code>","text":"<p>Scrapes play-by-play data for a game.</p> <p>Parameters: - <code>game</code> (str | int): Game ID - <code>addGoalReplayData</code> (bool): Include goal replay data - <code>output_format</code> (str): Output format</p> <p>Returns: - <code>pd.DataFrame</code> or <code>pl.DataFrame</code>: Play-by-play data</p> <p>Example: <pre><code>from scrapernhl.scrapers.games import scrapePlays\n\npbp = scrapePlays(2024020001)\n</code></pre></p>"},{"location":"api/#getgoalreplaydatajson_url-str-listdict","title":"<code>getGoalReplayData(json_url: str) -&gt; List[Dict]</code>","text":"<p>Fetches goal replay data from a JSON URL.</p> <p>Parameters: - <code>json_url</code> (str): URL to goal replay JSON</p> <p>Returns: - <code>List[Dict]</code>: Goal replay data</p>"},{"location":"api/#convert_json_to_goal_urljson_url-str-str","title":"<code>convert_json_to_goal_url(json_url: str) -&gt; str</code>","text":"<p>Converts a JSON URL to NHL goal replay URL.</p> <p>Parameters: - <code>json_url</code> (str): JSON URL</p> <p>Returns: - <code>str</code>: Goal replay URL</p>"},{"location":"api/#core-utilities-scrapernhlcore","title":"Core Utilities (<code>scrapernhl.core</code>)","text":""},{"location":"api/#http-scrapernhlcorehttp","title":"HTTP (<code>scrapernhl.core.http</code>)","text":""},{"location":"api/#fetch_jsonurl-str-timeout-int-10-dict","title":"<code>fetch_json(url: str, timeout: int = 10) -&gt; dict</code>","text":"<p>Fetches JSON data from a URL with retry logic.</p> <p>Parameters: - <code>url</code> (str): URL to fetch - <code>timeout</code> (int): Request timeout in seconds</p> <p>Returns: - <code>dict</code>: Parsed JSON response</p> <p>Raises: - <code>requests.exceptions.RequestException</code>: If request fails</p> <p>Example: <pre><code>from scrapernhl.core.http import fetch_json\n\ndata = fetch_json(\"https://api-web.nhle.com/v1/...\")\n</code></pre></p>"},{"location":"api/#fetch_htmlurl-str-timeout-int-10-optionalstr","title":"<code>fetch_html(url: str, timeout: int = 10) -&gt; Optional[str]</code>","text":"<p>Fetches HTML content from a URL.</p> <p>Parameters: - <code>url</code> (str): URL to fetch - <code>timeout</code> (int): Request timeout in seconds</p> <p>Returns: - <code>str</code> or <code>None</code>: HTML content, or None if request fails</p>"},{"location":"api/#fetch_html_asyncurl-str-timeout-int-10-optionalstr","title":"<code>fetch_html_async(url: str, timeout: int = 10) -&gt; Optional[str]</code>","text":"<p>Async wrapper around fetch_html.</p>"},{"location":"api/#fetch_json_asyncurl-str-timeout-int-10-dict","title":"<code>fetch_json_async(url: str, timeout: int = 10) -&gt; dict</code>","text":"<p>Async wrapper around fetch_json.</p>"},{"location":"api/#utils-scrapernhlcoreutils","title":"Utils (<code>scrapernhl.core.utils</code>)","text":""},{"location":"api/#time_str_to_secondstime_str-optionalstr-optionalint","title":"<code>time_str_to_seconds(time_str: Optional[str]) -&gt; Optional[int]</code>","text":"<p>Converts time string in 'MM:SS' format to total seconds.</p> <p>Parameters: - <code>time_str</code> (str): Time string (e.g., \"05:30\")</p> <p>Returns: - <code>int</code> or <code>None</code>: Total seconds</p> <p>Example: <pre><code>from scrapernhl.core.utils import time_str_to_seconds\n\nseconds = time_str_to_seconds(\"05:30\")  # Returns 330\n</code></pre></p>"},{"location":"api/#json_normalizedata-listdict-output_format-str-pandas-dataframe","title":"<code>json_normalize(data: List[Dict], output_format: str = \"pandas\") -&gt; DataFrame</code>","text":"<p>Normalizes nested JSON data to flat table.</p> <p>Parameters: - <code>data</code> (List[Dict]): List of dictionaries - <code>output_format</code> (str): \"pandas\" or \"polars\"</p> <p>Returns: - <code>pd.DataFrame</code> or <code>pl.DataFrame</code>: Normalized data</p>"},{"location":"api/#configuration-scrapernhlconfig","title":"Configuration (<code>scrapernhl.config</code>)","text":""},{"location":"api/#constants","title":"Constants","text":"<pre><code>from scrapernhl.config import (\n    DEFAULT_TEAM,      # \"MTL\"\n    DEFAULT_SEASON,    # \"20252026\"\n    DEFAULT_DATE,      # \"2025-11-11\"\n    DEFAULT_HEADERS,   # HTTP headers dict\n    DEFAULT_TIMEOUT,   # 10 seconds\n)\n</code></pre>"},{"location":"api/#api-endpoints","title":"API Endpoints","text":"<pre><code>from scrapernhl.config import (\n    NHL_API_BASE_URL,\n    NHL_API_BASE_URL_V1,\n    STANDINGS_ENDPOINT,\n    TEAM_SCHEDULE_ENDPOINT,\n    FRANCHISES_ENDPOINT,\n)\n</code></pre>"},{"location":"api/#legacy-functions","title":"Legacy Functions","text":"<p>Advanced functions (PBP parsing, feature engineering, analytics) are available via lazy loading from <code>scraper_legacy.py</code>. These load only when accessed:</p> <pre><code>from scrapernhl import (\n    scrape_game,\n    engineer_xg_features,\n    build_on_ice_wide,\n    toi_by_strength,\n    combo_on_ice_stats,\n)\n</code></pre> <p>Note: These functions require additional dependencies (xgboost, etc.) and are being modularized in Phase 2.</p>"},{"location":"api/#complete-example","title":"Complete Example","text":"<pre><code>from scrapernhl.scrapers.teams import scrapeTeams\nfrom scrapernhl.scrapers.schedule import scrapeSchedule\nfrom scrapernhl.scrapers.standings import scrapeStandings\nfrom scrapernhl.scrapers.games import scrapePlays\n\n# Get all teams\nteams = scrapeTeams()\nprint(f\"Found {len(teams)} teams\")\n\n# Get Montreal Canadiens schedule\nmtl_schedule = scrapeSchedule(\"MTL\", \"20252026\")\nprint(f\"MTL has {len(mtl_schedule)} games\")\n\n# Get current standings\nstandings = scrapeStandings(\"2025-01-01\")\nprint(f\"Standings for {len(standings)} teams\")\n\n# Get play-by-play for a specific game\npbp = scrapePlays(2024020001)\nprint(f\"Game has {len(pbp)} events\")\n</code></pre>"},{"location":"api/#performance-tips","title":"Performance Tips","text":"<ol> <li> <p>Use modular imports for faster loading:    <pre><code>from scrapernhl.scrapers.teams import scrapeTeams  # Fast\nfrom scrapernhl import scrapeTeams  # Also fast, but loads more\n</code></pre></p> </li> <li> <p>Choose output format based on your needs:    <pre><code>teams_pd = scrapeTeams(output_format=\"pandas\")   # pandas\nteams_pl = scrapeTeams(output_format=\"polars\")   # polars (faster)\n</code></pre></p> </li> <li> <p>Cache results when scraping multiple times:    <pre><code>@lru_cache(maxsize=100)\ndef get_cached_teams():\n    return scrapeTeams()\n</code></pre></p> </li> </ol> <p>For more information, see: - Getting Started Guide - Examples - Modularization Guide</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Install the package and start scraping NHL data in minutes.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>You can install the package using <code>uv</code> (recommended) with the following command: <pre><code>uv add scrapernhl\n</code></pre></p> <p>or via <code>pip</code>: <pre><code>pip install scrapernhl\n</code></pre></p> <p>or from GitHub (for the latest version): <pre><code>pip install git+https://github.com/max-tixador/scrapernhl.git\n</code></pre></p>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":""},{"location":"getting-started/#command-line-interface","title":"Command-Line Interface","text":"<p>The fastest way to get started is using the CLI:</p> <pre><code># Get help\npython scrapernhl/cli.py --help\n\n# Scrape all NHL teams\npython scrapernhl/cli.py teams\n\n# Get a team's schedule\npython scrapernhl/cli.py schedule MTL 20252026\n\n# Get current standings\npython scrapernhl/cli.py standings\n</code></pre> <p>See CLI Examples for more examples.</p>"},{"location":"getting-started/#python-api","title":"Python API","text":"<pre><code>from scrapernhl import *\n\n# Scrape teams\nteams = scrape_teams()\n\n# Scrape schedule\nschedule = scrape_schedule('MTL', '20252026')\n\n# Scrape standings\nstandings = scrape_standings('2026-01-01')\n</code></pre> <p>See API Reference for all available functions.</p>"},{"location":"getting-started/#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.12</li> <li>See full dependencies in <code>pyproject.toml</code></li> </ul>"},{"location":"announcements/version-014/","title":"ANNOUNCEMENT: Version 0.1.4 Released","text":"<p>Date: January 1st, 2026</p> <p>I am delighted to announce the release of version 0.1.4 of the scraper package! This update brings several new features, improvements, and bug fixes that enhance the overall user experience.</p> <p>Most importantly, this release marks a significant milestone with the introduction of a comprehensive documentation website. The new documentation provides detailed information on how to use the scraper, including tutorials, examples, and API references.</p> <p>I worked extremely hard on this release, focusing on modularizing the codebase, standardizing the code style, and adding unit tests to ensure reliability. The monolithic 5000+ line scraper.py has been split into focused, single-responsibility modules organized by function (scrapers, core utilities, features, analysis). Additionally, I have integrated command-line interface (CLI) support, allowing users to run the scraper directly from the terminal.</p> <p>I have also included new tutorials and examples to help users get started quickly and make the most of the scraper's capabilities. The new modular structure provides faster imports (~100ms vs 2-3s previously), better code organization, and maintains 100% backward compatibility with existing code.</p> <p>Don't hesitate to give feedback or report any issues you encounter. Your input is invaluable in helping me improve the scraper further.</p> <p>Finally, I encourage developers to contribute to the project. Whether it's through code contributions, documentation improvements, or feature suggestions, your involvement is greatly appreciated. Let's make this scraper the best it can be to encourage public research in hockey analytics!</p>","tags":["announcement","release"]},{"location":"announcements/version-014/#new-features","title":"New Features","text":"","tags":["announcement","release"]},{"location":"announcements/version-014/#summary-of-new-features","title":"Summary of New Features","text":"<ul> <li>Documentation: There is finally a documentation website!</li> <li>Modularization: The codebase has been completely restructured from a single 5000+ line file into focused modules:</li> <li><code>scrapernhl.scrapers.*</code> - Individual data scrapers (teams, schedule, standings, roster, stats, draft, games)</li> <li><code>scrapernhl.core.*</code> - Core utilities (HTTP fetching with retry logic, helper functions)</li> <li><code>scrapernhl.config</code> - Centralized configuration and API endpoints</li> <li>Original code (with minor improvements) safely backed up as <code>scraper_legacy.py</code></li> <li>100% backward compatible - all existing code works without changes</li> <li>Lazy loading - heavy dependencies (xgboost) only load when needed</li> <li>Faster imports - basic scrapers load in ~100ms (vs 2-3s previously)</li> <li>Standardization: Code style has been standardized across the project to improve readability and consistency.</li> <li>Testing: Comprehensive unit tests have been added (<code>tests/test_modular.py</code>) to ensure code reliability and facilitate future development.</li> <li>CLI Integration: You can now run the scraper directly from the command line interface (CLI) for easier access and faster execution.</li> <li>Tutorials and Examples: New tutorials and examples have been added to help users get started and make the most of the scraper's capabilities. </li> <li>Performance Improvements: Optimizations have been made to improve the speed and efficiency of the scraper.</li> </ul>","tags":["announcement","release"]},{"location":"announcements/version-014/#1-documentation-website","title":"1. Documentation Website","text":"<p>The new documentation website provides comprehensive information on how to use the scraper, including tutorials, examples, and API references. It is designed to help users quickly get started and make the most of the scraper's capabilities. You can access the documentation here.</p>","tags":["announcement","release"]},{"location":"announcements/version-014/#2-modular-codebase","title":"2. Modular Codebase","text":"<p>The codebase has been restructured into focused modules, each responsible for a specific aspect of the scraper's functionality. This modular approach improves code organization, readability, and maintainability. The original monolithic code has been preserved in <code>scraper_legacy.py</code> for reference.</p> <p>I have ensured that all existing code remains fully functional without any changes, maintaining 100% backward compatibility. Additionally, the modular structure allows for faster imports, with basic scrapers loading in approximately 100 milliseconds compared to 2-3 seconds previously.</p>","tags":["announcement","release"]},{"location":"announcements/version-014/#3-standardized-code-style","title":"3. Standardized Code Style","text":"<p>To enhance readability and consistency across the project, I have standardized the code style. This includes adhering to best practices and conventions, making it easier for developers to understand and contribute to the codebase. I don't use black or flake8 yet, but I may consider it in future releases. Let me know if you have strong opinions on this!</p>","tags":["announcement","release"]},{"location":"announcements/version-014/#4-comprehensive-testing","title":"4. Comprehensive Testing","text":"<p>I have added comprehensive unit tests in <code>tests/test_modular.py</code> to ensure the reliability of the codebase. These tests cover various aspects of the scraper's functionality and help identify potential issues early in the development process. The testing framework facilitates future development and ensures that new features do not introduce regressions.</p> <p>Testing has always been a weak point of mine, so I am particularly proud of this addition! I think there are still some edge cases that need to be covered, so please report any bugs you find. I could also use help writing more tests if anyone is interested.</p>","tags":["announcement","release"]},{"location":"announcements/version-014/#5-cli-integration","title":"5. CLI Integration","text":"<p>The scraper can now be run directly from the command line interface (CLI), providing users with easier access and faster execution. This feature allows users to quickly scrape data without needing to write additional code.</p> <p>The CLI supports all major scraping functions with flexible output formats (CSV, JSON, Parquet, Excel):</p> <pre><code># Get help\npython scrapernhl/cli.py --help\n\n# Scrape all NHL teams\npython scrapernhl/cli.py teams --output nhl_teams.csv\n\n# Scrape team schedule\npython scrapernhl/cli.py schedule MTL 20252026 --output mtl_schedule.json --format json\n\n# Scrape current standings\npython scrapernhl/cli.py standings\n\n# Scrape team roster\npython scrapernhl/cli.py roster TOR 20252026\n\n# Scrape player stats (add --goalies for goalie stats)\npython scrapernhl/cli.py stats MTL 20252026 --output mtl_skaters.csv\n\n# Scrape game play-by-play (add --with-xg for expected goals)\npython scrapernhl/cli.py game 2024020001 --with-xg\n\n# Scrape draft data\npython scrapernhl/cli.py draft 2024 1  # First round only\npython scrapernhl/cli.py draft 2024 all  # All rounds\n</code></pre> <p>The CLI makes it easy to integrate NHL data scraping into shell scripts, cron jobs, or automated workflows without writing any Python code.</p>","tags":["announcement","release"]},{"location":"announcements/version-014/#6-tutorials-and-examples","title":"6. Tutorials and Examples","text":"<p>To help users get started and make the most of the scraper's capabilities, I have added new tutorials and examples. These resources cover various use cases and demonstrate how to effectively utilize the scraper for hockey analytics. You can find the examples in the <code>docs/examples</code> directory of the documentation website.</p> <p>For now, there are no exemples for data visualization, but I plan to add some in the future!</p>","tags":["announcement","release"]},{"location":"announcements/version-014/#future-plans","title":"Future Plans","text":"<p>Looking ahead, we plan to continue enhancing the scraper with additional features and improvements. Future updates may include:</p> <ul> <li>Arena Adjusted Event Coordinates: Implementing arena-adjusted event coordinates for more accurate data representation.</li> <li>Data Visualization Tools: Adding tools for visualizing scraped data to facilitate analysis and insights.</li> <li>Manual Data Input: Enabling users to manually input data for scenarios where automated scraping may not be feasible.</li> <li>Docker Support: Introducing Docker support for easier deployment and environment management.</li> <li>Database Integration: Scraping data all the time can be resource-intensive. I will explore creating a database people can connect to instead of scraping on their own machines.</li> <li>New Leagues: Expanding support to include additional hockey leagues (looking closely at the PWHL) beyond the NHL.</li> <li><code>Dev module</code>: A new development module to streamline the development process and enhance collaboration. Same ScraperNHL functions, but with additional logging and error handling for debugging purposes and extra features (runtime, caching, messages) for extended development.</li> <li>Improved Error Handling: Enhanced error handling mechanisms to provide more informative feedback and improve user experience.</li> <li>More Examples and Tutorials: Continuing to add more examples and tutorials to help users get the most out of the scraper. I don't think I will wait for another major release to add these!</li> </ul> <p>By Max, your favorite hockey analytics enthusiast</p>","tags":["announcement","release"]},{"location":"examples/advanced/","title":"Advanced Examples","text":"<p>Advanced usage patterns for data analysis and feature engineering.</p>"},{"location":"examples/advanced/#find-a-recent-completed-game","title":"Find a Recent Completed Game","text":"<pre><code>from scrapernhl.scrapers.schedule import scrapeSchedule\nfrom datetime import datetime\n\n# Get recent games from any team\nschedule = scrapeSchedule(\"MTL\", \"20252026\")\ncompleted = schedule[schedule['gameState'] == 'OFF']\n\nif len(completed) &gt; 0:\n    game_id = completed.iloc[0]['id']\n    game_info = completed.iloc[0]\n    print(f\"Using game: {game_info['awayTeam.abbrev']} @ {game_info['homeTeam.abbrev']}\")\n    print(f\"Date: {game_info['gameDate']}\")\n    print(f\"Game ID: {game_id}\")\nelse:\n    print(\"No completed games found. Using a known game ID...\")\n    game_id = 2024020001\n</code></pre>"},{"location":"examples/advanced/#working-with-complete-game-data","title":"Working with Complete Game Data","text":"<pre><code>from scrapernhl import scrape_game\n\n# Get comprehensive game data\ngame_tuple = scrape_game(game_id=game_id, include_tuple=True)\n\npbp = game_tuple.data\n\nprint(f\"Game: {game_tuple.awayTeam} @ {game_tuple.homeTeam}\")\nprint(f\"Total events: {len(pbp)}\")\n\nprint(\"\\nRosters:\")\ngame_tuple.rosters\n</code></pre>"},{"location":"examples/advanced/#expected-goals-xg-features","title":"Expected Goals (xG) Features","text":"<pre><code>from scrapernhl import engineer_xg_features, predict_xg_for_pbp\n\n# Engineer xG features\npbp_with_features = engineer_xg_features(pbp)\n\n# Predict xG\npbp_with_xg = predict_xg_for_pbp(pbp_with_features)\n\n# Get shots and goals\nshots = pbp_with_xg[pbp_with_xg['Event'].isin(['SHOT', 'GOAL', 'MISS'])].copy()\n\nprint(f\"Total shot attempts: {len(shots)}\")\n\nprint(\"\\nShot attempts with xG:\")\nshots[['period', 'timeInPeriod', 'Event', 'eventTeam', 'player1Id', 'xG', 'distanceFromGoal']].head(10)\n\n# Calculate team xG totals\nhome_team = game_tuple.homeTeam\naway_team = game_tuple.awayTeam\n\nhome_shots = shots[shots['eventTeam'] == home_team]\naway_shots = shots[shots['eventTeam'] == away_team]\n\nhome_xg = home_shots['xG'].sum()\naway_xg = away_shots['xG'].sum()\n\nhome_goals = len(pbp[(pbp['Event'] == 'GOAL') &amp; (pbp['eventTeam'] == home_team)])\naway_goals = len(pbp[(pbp['Event'] == 'GOAL') &amp; (pbp['eventTeam'] == away_team)])\n\nprint(f\"\\n{away_team} @ {home_team}\")\nprint(f\"Score: {away_goals} - {home_goals}\")\nprint(f\"xG: {away_xg:.2f} - {home_xg:.2f}\")\nprint(f\"xG differential: {home_xg - away_xg:.2f} (positive favors {home_team})\")\n</code></pre>"},{"location":"examples/advanced/#time-on-ice-toi-analysis","title":"Time on Ice (TOI) Analysis","text":"<pre><code>from scrapernhl import toi_by_strength\n\n# Calculate TOI by strength\ntoi_df = toi_by_strength(pbp)\n\nprint(\"TOI by strength:\")\ntoi_df.head(10)\n\n# Calculate individual player stats\nfrom scrapernhl import combo_on_ice_stats_both_teams\n\ncombo_stats = combo_on_ice_stats_both_teams(\n    pbp,\n    n_team=1,\n    m_opp=0,          # set 0 for \"vs ANY\"\n    min_TOI=0,\n    include_goalies=False,\n    rates=True,\n    player_df=game_tuple.rosters  # DataFrame with ids/teams/positions\n)\n\ncombo_stats[['player1Id', 'player1Name', 'player1Position', 'player1Number', 'team', 'opp', 'strength', 'seconds', 'minutes']]\n</code></pre>"},{"location":"examples/advanced/#player-combinations-analysis","title":"Player Combinations Analysis","text":""},{"location":"examples/advanced/#defensive-pairs-2-player-combinations","title":"Defensive Pairs (2-player combinations)","text":"<pre><code># Get 2-player combinations (defensive pairs)\ncombo_stats_2 = combo_on_ice_stats_both_teams(\n    pbp,\n    n_team=2,\n    m_opp=0,          # set 0 for \"vs ANY\"\n    min_TOI=60,\n    include_goalies=False,\n    rates=True,\n    player_df=game_tuple.rosters  # DataFrame with ids/teams/positions\n)\n\ntop_10_pairs_5v5 = (combo_stats_2\n                    .query(\"team_combo_pos == '2D'\")  # Get defensive pairs\n                    .query(\"strength == '5v5'\")  # Filter for 5v5\n                    .nlargest(10, 'seconds')\n                    )[['team_combo', 'team_combo_ids', 'team', 'opp', 'strength', 'seconds', 'minutes']]\n\nprint(\"Most common defensive pairs (5v5):\")\ntop_10_pairs_5v5\n</code></pre>"},{"location":"examples/advanced/#forward-lines-3-player-combinations","title":"Forward Lines (3-player combinations)","text":"<pre><code># Get 3-player combinations (forward lines)\ncombo_stats_3 = combo_on_ice_stats_both_teams(\n    pbp,\n    n_team=3,\n    m_opp=0,          # set 0 for \"vs ANY\"\n    min_TOI=60,\n    include_goalies=False,\n    rates=True,\n    player_df=game_tuple.rosters  # DataFrame with ids/teams/positions\n)\n\ntop_10_lines_5v5 = (combo_stats_3\n                    .query(\"team_combo_pos == '3F'\")  # Get offensive lines\n                    .query(\"strength == '5v5'\")  # Filter for 5v5\n                    .nlargest(10, 'seconds')\n                    )[['team_combo', 'team_combo_ids', 'team', 'opp', 'strength', 'seconds', 'minutes']]\n\nprint(\"Most common offensive lines (5v5):\")\ntop_10_lines_5v5\n</code></pre>"},{"location":"examples/advanced/#on-ice-statistics-by-player","title":"On-Ice Statistics by Player","text":"<pre><code>from scrapernhl import scrape_game, on_ice_stats_by_player_strength\n\n# Get game with xG\npbp, _ = scrape_game(2024020001)\n\n# Calculate on-ice stats for each player\nplayer_stats = on_ice_stats_by_player_strength(\n    pbp,\n    include_goalies=False,\n    rates=True  # Convert to per-60 rates\n)\n\n# Show top players by Corsi For %\nprint(\"Best Corsi For % (min 5 min TOI):\")\ncf_leaders = player_stats[player_stats['TOI'] &gt;= 5].nlargest(10, 'CF%')\nprint(cf_leaders[['player', 'team', 'strength', 'TOI', 'CF', 'CA', 'CF%']])\n\n# Show players with best xG differential\nprint(\"\\\\nBest xG differential:\")\nxg_leaders = player_stats[player_stats['TOI'] &gt;= 5]\nxg_leaders['xG_diff'] = xg_leaders['xG'] - xg_leaders['xGA']\nprint(xg_leaders.nlargest(10, 'xG_diff')[['player', 'team', 'xG', 'xGA', 'xG_diff']])\n</code></pre>"},{"location":"examples/advanced/#team-level-aggregates","title":"Team-Level Aggregates","text":"<pre><code>from scrapernhl import team_strength_aggregates\n\n# Calculate team stats by strength\nteam_stats = team_strength_aggregates(\n    pbp_with_xg,\n    include_goalies=False,\n    rates=True,\n    min_TOI=1\n)\n\nprint(\"Team statistics by strength:\")\nteam_stats[['team', 'minutes', 'CF', 'CA', 'xG', 'xGA', 'GF', 'GA']].sort_values(by=['minutes'], ascending=False)\n\n# 5v5 stats only\nstats_5v5 = team_stats[team_stats['strength'] == '5v5'].copy()\n\nprint(\"\\n5v5 Team Stats:\")\nstats_5v5[['team', 'minutes', 'CF', 'CA', 'xG', 'xGA', 'GF', 'GA']]\n</code></pre>"},{"location":"examples/advanced/#multi-game-season-analysis","title":"Multi-Game Season Analysis","text":"<pre><code>import pandas as pd\n\n# Scrape multiple games (just 3 for demonstration)\nprint(\"Scraping multiple games for season analysis...\")\n\ngame_ids_to_scrape = completed.head(3)['id'].tolist()\nall_team_stats = []\n\nfor gid in game_ids_to_scrape:\n    try:\n        print(f\"Processing game {gid}...\")\n        game_tuple = scrape_game(gid, include_tuple=True)\n        pbp = game_tuple.data\n        pbp = engineer_xg_features(pbp)\n        pbp = predict_xg_for_pbp(pbp)\n\n        stats = team_strength_aggregates(pbp)\n        stats['game_id'] = gid\n        all_team_stats.append(stats)\n    except Exception as e:\n        print(f\"Error with game {gid}: {e}\")\n\n# Combine all games\nif all_team_stats:\n    season_stats = pd.concat(all_team_stats, ignore_index=True)\n\n    # Aggregate by team\n    team_summary = season_stats.groupby('team').agg({\n        'minutes': 'sum',\n        'CF': 'sum',\n        'CA': 'sum',\n        'xG': 'sum',\n        'xGA': 'sum',\n        'GF': 'sum',\n        'GA': 'sum'\n    }).reset_index()\n\n    team_summary['CF%'] = 100 * team_summary['CF'] / (team_summary['CF'] + team_summary['CA'])\n\n    print(\"\\\\nSeason stats across sampled games:\")\n    team_summary\n</code></pre> <ul> <li>API Reference - Complete function documentation</li> </ul>"},{"location":"examples/cli/","title":"Command-Line Interface (CLI) Examples","text":"<p>The ScraperNHL CLI allows you to scrape NHL data directly from the command line without writing any Python code. This is perfect for quick data exports, shell scripts, cron jobs, and automated workflows.</p>"},{"location":"examples/cli/#getting-started","title":"Getting Started","text":"<p>View available commands:</p> <pre><code>python scrapernhl/cli.py --help\n</code></pre> <p>Output: <pre><code>Usage: cli.py [OPTIONS] COMMAND [ARGS]...\n\n  ScraperNHL - Command-line interface for NHL data scraping.\n\n  Scrape NHL teams, schedules, standings, rosters, stats, games, and draft\n  data directly from the command line.\n\nOptions:\n  --version  Show the version and exit.\n  --help     Show this message and exit.\n\nCommands:\n  draft      Scrape NHL draft data.\n  game       Scrape play-by-play data for a specific game.\n  roster     Scrape team roster.\n  schedule   Scrape team schedule.\n  standings  Scrape NHL standings.\n  stats      Scrape team player statistics.\n  teams      Scrape all NHL teams.\n</code></pre></p>"},{"location":"examples/cli/#output-formats","title":"Output Formats","text":"<p>All commands support multiple output formats via the <code>--format</code> flag:</p> <ul> <li><code>csv</code> (default) - Comma-separated values</li> <li><code>json</code> - JSON format</li> <li><code>parquet</code> - Compressed Parquet format (best for large datasets)</li> <li><code>excel</code> - Excel spreadsheet (.xlsx)</li> </ul> <p>Example: <pre><code>python scrapernhl/cli.py teams --format json --output teams.json\n</code></pre></p>"},{"location":"examples/cli/#teams-command","title":"Teams Command","text":"<p>Scrape information about all NHL teams.</p>"},{"location":"examples/cli/#basic-usage","title":"Basic Usage","text":"<pre><code># Default: saves to nhl_teams.csv\npython scrapernhl/cli.py teams\n\n# Specify custom output file\npython scrapernhl/cli.py teams --output my_teams.csv\n\n# Export as JSON\npython scrapernhl/cli.py teams --format json --output teams.json\n\n# Export as Parquet\npython scrapernhl/cli.py teams --format parquet --output teams.parquet\n\n# Use Polars instead of Pandas (faster for large datasets)\npython scrapernhl/cli.py teams --polars\n</code></pre>"},{"location":"examples/cli/#example-output","title":"Example Output","text":"<pre><code>Scraping NHL teams...\nSuccessfully scraped 32 teams\nSaved to: nhl_teams.csv\n</code></pre>"},{"location":"examples/cli/#schedule-command","title":"Schedule Command","text":"<p>Scrape a team's schedule for a specific season.</p>"},{"location":"examples/cli/#basic-usage_1","title":"Basic Usage","text":"<pre><code>python scrapernhl/cli.py schedule TEAM SEASON [OPTIONS]\n</code></pre> <p>Arguments: - <code>TEAM</code> - Team abbreviation (e.g., MTL, TOR, BOS, NYR) - <code>SEASON</code> - Season in format YYYYYYYY (e.g., 20252026)</p>"},{"location":"examples/cli/#examples","title":"Examples","text":"<pre><code># Montreal Canadiens 2025-26 schedule\npython scrapernhl/cli.py schedule MTL 20252026\n\n# Toronto Maple Leafs with custom output\npython scrapernhl/cli.py schedule TOR 20252026 --output tor_schedule.csv\n\n# Boston Bruins as JSON\npython scrapernhl/cli.py schedule BOS 20252026 --format json\n\n# Vegas Golden Knights as Parquet\npython scrapernhl/cli.py schedule VGK 20252026 --format parquet\n</code></pre>"},{"location":"examples/cli/#example-output_1","title":"Example Output","text":"<pre><code>Scraping MTL schedule for 20252026...\nSuccessfully scraped 82 games\nSaved to: mtl_schedule_20252026.csv\n</code></pre>"},{"location":"examples/cli/#standings-command","title":"Standings Command","text":"<p>Scrape NHL standings for a specific date.</p>"},{"location":"examples/cli/#basic-usage_2","title":"Basic Usage","text":"<pre><code># Current standings (uses today's date)\npython scrapernhl/cli.py standings\n\n# Standings for specific date (YYYY-MM-DD)\npython scrapernhl/cli.py standings 2025-12-25\n\n# Export to different format\npython scrapernhl/cli.py standings --format json --output standings.json\n</code></pre>"},{"location":"examples/cli/#examples_1","title":"Examples","text":"<pre><code># Get current standings\npython scrapernhl/cli.py standings\n\n# Standings on Christmas 2025\npython scrapernhl/cli.py standings 2025-12-25\n\n# Save to custom file\npython scrapernhl/cli.py standings --output current_standings.csv\n</code></pre>"},{"location":"examples/cli/#example-output_2","title":"Example Output","text":"<pre><code>Scraping NHL standings for 2026-01-01...\nSuccessfully scraped standings for 32 teams\nSaved to: nhl_standings_2026-01-01.csv\n</code></pre>"},{"location":"examples/cli/#roster-command","title":"Roster Command","text":"<p>Scrape a team's roster for a specific season.</p>"},{"location":"examples/cli/#basic-usage_3","title":"Basic Usage","text":"<pre><code>python scrapernhl/cli.py roster TEAM SEASON [OPTIONS]\n</code></pre>"},{"location":"examples/cli/#examples_2","title":"Examples","text":"<pre><code># Montreal Canadiens roster\npython scrapernhl/cli.py roster MTL 20252026\n\n# Edmonton Oilers roster as JSON\npython scrapernhl/cli.py roster EDM 20252026 --format json\n\n# Custom output file\npython scrapernhl/cli.py roster TOR 20252026 --output leafs_roster.csv\n</code></pre>"},{"location":"examples/cli/#example-output_3","title":"Example Output","text":"<pre><code>Scraping MTL roster for 20252026...\nSuccessfully scraped 28 players\nSaved to: mtl_roster_20252026.csv\n</code></pre>"},{"location":"examples/cli/#stats-command","title":"Stats Command","text":"<p>Scrape player statistics for a team.</p>"},{"location":"examples/cli/#basic-usage_4","title":"Basic Usage","text":"<pre><code>python scrapernhl/cli.py stats TEAM SEASON [OPTIONS]\n</code></pre>"},{"location":"examples/cli/#options","title":"Options","text":"<ul> <li><code>--goalies</code> - Scrape goalie stats instead of skater stats</li> <li><code>--session</code> - Session type (2=regular season, 3=playoffs, default: 2)</li> </ul>"},{"location":"examples/cli/#examples_3","title":"Examples","text":"<pre><code># Skater stats (default)\npython scrapernhl/cli.py stats MTL 20252026\n\n# Goalie stats\npython scrapernhl/cli.py stats MTL 20252026 --goalies\n\n# Playoff stats\npython scrapernhl/cli.py stats TOR 20252026 --session 3\n\n# Combined example: playoff goalie stats\npython scrapernhl/cli.py stats BOS 20252026 --goalies --session 3\n\n# Export to JSON\npython scrapernhl/cli.py stats NYR 20252026 --format json\n</code></pre>"},{"location":"examples/cli/#example-output_4","title":"Example Output","text":"<pre><code>Scraping MTL skaters stats for 20252026...\nSuccessfully scraped stats for 23 skaters\nSaved to: mtl_skaters_20252026.csv\n</code></pre>"},{"location":"examples/cli/#game-command","title":"Game Command","text":"<p>Scrape play-by-play data for a specific game.</p>"},{"location":"examples/cli/#basic-usage_5","title":"Basic Usage","text":"<pre><code>python scrapernhl/cli.py game GAME_ID [OPTIONS]\n</code></pre> <p>Arguments: - <code>GAME_ID</code> - NHL game ID (e.g., 2024020001)</p>"},{"location":"examples/cli/#options_1","title":"Options","text":"<ul> <li><code>--with-xg</code> - Include expected goals (xG) predictions for shot events</li> </ul>"},{"location":"examples/cli/#examples_4","title":"Examples","text":"<pre><code># Basic play-by-play\npython scrapernhl/cli.py game 2024020001\n\n# With expected goals analysis\npython scrapernhl/cli.py game 2024020001 --with-xg\n\n# Export to JSON\npython scrapernhl/cli.py game 2024020001 --format json\n\n# Custom output with xG\npython scrapernhl/cli.py game 2024020001 --with-xg --output game_with_xg.parquet --format parquet\n</code></pre>"},{"location":"examples/cli/#example-output_5","title":"Example Output","text":"<pre><code>Scraping play-by-play for game 2024020001...\nSuccessfully scraped 312 events\nSaved to: game_2024020001.csv\n</code></pre> <p>With xG: <pre><code>Scraping play-by-play for game 2024020001...\nCalculated xG for shot events\nSuccessfully scraped 312 events\nSaved to: game_2024020001_with_xg.csv\n</code></pre></p>"},{"location":"examples/cli/#draft-command","title":"Draft Command","text":"<p>Scrape NHL draft data.</p>"},{"location":"examples/cli/#basic-usage_6","title":"Basic Usage","text":"<pre><code>python scrapernhl/cli.py draft YEAR [ROUND] [OPTIONS]\n</code></pre> <p>Arguments: - <code>YEAR</code> - Draft year (e.g., 2024, 2025) - <code>ROUND</code> - Draft round (1-7 or 'all', default: all)</p>"},{"location":"examples/cli/#examples_5","title":"Examples","text":"<pre><code># All rounds\npython scrapernhl/cli.py draft 2024\n\n# First round only\npython scrapernhl/cli.py draft 2024 1\n\n# Specific round\npython scrapernhl/cli.py draft 2024 2\n\n# All rounds as JSON\npython scrapernhl/cli.py draft 2025 all --format json\n\n# Custom output\npython scrapernhl/cli.py draft 2024 1 --output first_round_2024.csv\n</code></pre>"},{"location":"examples/cli/#example-output_6","title":"Example Output","text":"<pre><code>Scraping 2024 NHL draft (round 1)...\nSuccessfully scraped 32 draft picks\nSaved to: nhl_draft_2024_r1.csv\n</code></pre>"},{"location":"examples/cli/#practical-use-cases","title":"Practical Use Cases","text":""},{"location":"examples/cli/#daily-standings-report","title":"Daily Standings Report","text":"<p>Create a shell script to get daily standings:</p> <pre><code>#!/bin/bash\n# daily_standings.sh\n\nDATE=$(date +%Y-%m-%d)\npython scrapernhl/cli.py standings $DATE --output \"standings_$DATE.csv\"\necho \"Standings saved for $DATE\"\n</code></pre>"},{"location":"examples/cli/#team-data-export","title":"Team Data Export","text":"<p>Export all data for your favorite team:</p> <pre><code>#!/bin/bash\n# export_team_data.sh\n\nTEAM=\"MTL\"\nSEASON=\"20252026\"\n\npython scrapernhl/cli.py schedule $TEAM $SEASON --output ${TEAM}_schedule.csv\npython scrapernhl/cli.py roster $TEAM $SEASON --output ${TEAM}_roster.csv\npython scrapernhl/cli.py stats $TEAM $SEASON --output ${TEAM}_skaters.csv\npython scrapernhl/cli.py stats $TEAM $SEASON --goalies --output ${TEAM}_goalies.csv\n\necho \"All $TEAM data exported!\"\n</code></pre>"},{"location":"examples/cli/#automated-game-scraping","title":"Automated Game Scraping","text":"<p>Set up a cron job to scrape games after they finish:</p> <pre><code># Cron job: Run every hour during hockey season\n0 * * * * /path/to/scrape_recent_games.sh\n\n# scrape_recent_games.sh\n#!/bin/bash\nfor GAME_ID in 2024020100 2024020101 2024020102; do\n    python scrapernhl/cli.py game $GAME_ID --with-xg --format parquet\ndone\n</code></pre>"},{"location":"examples/cli/#multi-format-export","title":"Multi-Format Export","text":"<p>Export the same data in multiple formats:</p> <pre><code>#!/bin/bash\n# multi_format_export.sh\n\npython scrapernhl/cli.py teams --output teams.csv --format csv\npython scrapernhl/cli.py teams --output teams.json --format json\npython scrapernhl/cli.py teams --output teams.parquet --format parquet\n\necho \"Teams data exported in CSV, JSON, and Parquet formats\"\n</code></pre>"},{"location":"examples/cli/#tips-and-best-practices","title":"Tips and Best Practices","text":""},{"location":"examples/cli/#output-file-names","title":"Output File Names","text":"<p>If you don't specify <code>--output</code>, the CLI automatically generates descriptive filenames:</p> <ul> <li><code>teams</code> \u2192 <code>nhl_teams.csv</code></li> <li><code>schedule MTL 20252026</code> \u2192 <code>mtl_schedule_20252026.csv</code></li> <li><code>roster TOR 20252026</code> \u2192 <code>tor_roster_20252026.csv</code></li> <li><code>game 2024020001</code> \u2192 <code>game_2024020001.csv</code></li> </ul>"},{"location":"examples/cli/#format-selection","title":"Format Selection","text":"<p>Choose the right format for your use case:</p> <ul> <li>CSV - Best for Excel, simple analysis, human-readable</li> <li>JSON - Best for web apps, APIs, JavaScript</li> <li>Parquet - Best for large datasets, Python/pandas, analytics (50-80% smaller than CSV)</li> <li>Excel - Best for sharing with non-technical users</li> </ul>"},{"location":"examples/cli/#performance","title":"Performance","text":"<p>For faster processing of large datasets:</p> <ul> <li>Use <code>--polars</code> flag with the <code>teams</code> command</li> <li>Use <code>--format parquet</code> for better compression</li> <li>Process multiple files in parallel using shell background jobs (<code>&amp;</code>)</li> </ul>"},{"location":"examples/cli/#integration-with-python","title":"Integration with Python","text":"<p>You can mix CLI and Python usage:</p> <pre><code># Scrape with CLI\npython scrapernhl/cli.py schedule MTL 20252026 --output schedule.csv\n\n# Then analyze in Python\npython -c \"\nimport pandas as pd\nschedule = pd.read_csv('schedule.csv')\nprint(schedule[schedule['gameState'] == 'OFF'].head())\n\"\n</code></pre>"},{"location":"examples/cli/#see-also","title":"See Also","text":"<ul> <li>Basic Scraping Examples - Python API examples</li> <li>Advanced Examples - Data analysis</li> <li>Data Export - Exporting from Python</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"examples/export/","title":"Data Export Examples","text":"<p>Learn how to save scraped data to various file formats.</p>"},{"location":"examples/export/#setup","title":"Setup","text":"<pre><code>import pandas as pd\nimport os\nfrom datetime import datetime\n\n# Create output directory\nos.makedirs('output', exist_ok=True)\nprint(\"Created output/ directory for exported files\")\n</code></pre>"},{"location":"examples/export/#export-to-csv","title":"Export to CSV","text":"<pre><code>from scrapernhl.scrapers.teams import scrapeTeams\nfrom scrapernhl.scrapers.schedule import scrapeSchedule\n\n# Scrape teams\nteams = scrapeTeams()\nteams.to_csv('output/nhl_teams.csv', index=False)\nprint(f\"\u2705 Saved {len(teams)} teams to output/nhl_teams.csv\")\n\n# Scrape schedule\nschedule = scrapeSchedule(\"MTL\", \"20252026\")\nschedule.to_csv('output/mtl_schedule.csv', index=False)\nprint(f\"\u2705 Saved {len(schedule)} games to output/mtl_schedule.csv\")\n</code></pre>"},{"location":"examples/export/#export-to-excel-multiple-sheets","title":"Export to Excel (Multiple Sheets)","text":"<pre><code>from scrapernhl.scrapers.standings import scrapeStandings\nfrom scrapernhl.scrapers.stats import scrapeTeamStats\n\n# Scrape data\ntoday = datetime.now().strftime(\"%Y-%m-%d\")\nstandings = scrapeStandings(today)\nskaters = scrapeTeamStats(\"MTL\", \"20252026\", goalies=False)\ngoalies = scrapeTeamStats(\"MTL\", \"20252026\", goalies=True)\n\n# Save to Excel with multiple sheets\nwith pd.ExcelWriter('output/nhl_data.xlsx', engine='openpyxl') as writer:\n    standings.to_excel(writer, sheet_name='Standings', index=False)\n    skaters.to_excel(writer, sheet_name='Skaters', index=False)\n    goalies.to_excel(writer, sheet_name='Goalies', index=False)\n\nprint(f\"\u2705 Saved Excel file with 3 sheets to output/nhl_data.xlsx\")\nprint(f\"   - Standings: {len(standings)} teams\")\nprint(f\"   - Skaters: {len(skaters)} players\")\nprint(f\"   - Goalies: {len(goalies)} goalies\")\n</code></pre>"},{"location":"examples/export/#export-to-json","title":"Export to JSON","text":"<pre><code>from scrapernhl.scrapers.games import scrapePlays\n\n# Get a completed game\ncompleted_games = schedule[schedule['gameState'] == 'OFF']\nif len(completed_games) &gt; 0:\n    game_id = completed_games.iloc[0]['id']\n\n    # Scrape play-by-play\n    pbp = scrapePlays(game_id)\n\n    # Save as JSON (pretty printed)\n    pbp.to_json('output/game_pbp.json', orient='records', indent=2)\n    print(f\"\u2705 Saved {len(pbp)} events to output/game_pbp.json (pretty format)\")\n\n    # Save as JSON lines (more efficient for large files)\n    pbp.to_json('output/game_pbp.jsonl', orient='records', lines=True)\n    print(f\"\u2705 Saved {len(pbp)} events to output/game_pbp.jsonl (lines format)\")\n\n    # Compare file sizes\n    json_size = os.path.getsize('output/game_pbp.json') / 1024\n    jsonl_size = os.path.getsize('output/game_pbp.jsonl') / 1024\n    print(f\"   File sizes: JSON={json_size:.1f}KB, JSONL={jsonl_size:.1f}KB\")\nelse:\n    print(\"No completed games found\")\n</code></pre>"},{"location":"examples/export/#export-to-parquet-recommended-for-large-datasets","title":"Export to Parquet (Recommended for Large Datasets)","text":"<pre><code># Scrape multiple games\ngame_ids = completed_games.head(3)['id'].tolist() if len(completed_games) &gt;= 3 else []\n\nif game_ids:\n    all_pbp = []\n    for gid in game_ids:\n        print(f\"Scraping game {gid}...\")\n        pbp = scrapePlays(gid)\n        all_pbp.append(pbp)\n\n    # Combine\n    combined = pd.concat(all_pbp, ignore_index=True)\n\n    # Save as Parquet (compressed)\n    combined.to_parquet('output/games_pbp.parquet', index=False, compression='snappy')\n    print(f\"\u2705 Saved {len(combined)} events to output/games_pbp.parquet\")\n\n    # Read it back to verify\n    df = pd.read_parquet('output/games_pbp.parquet')\n    print(f\"\u2705 Verified: Loaded {len(df)} events from parquet\")\n\n    # Compare with CSV\n    combined.to_csv('output/games_pbp.csv', index=False)\n    parquet_size = os.path.getsize('output/games_pbp.parquet') / 1024\n    csv_size = os.path.getsize('output/games_pbp.csv') / 1024\n    print(f\"   File sizes: Parquet={parquet_size:.1f}KB, CSV={csv_size:.1f}KB\")\n    print(f\"   Compression: {100 * (1 - parquet_size/csv_size):.1f}% smaller\")\nelse:\n    print(\"No completed games available\")\n</code></pre>"},{"location":"examples/export/#export-with-polars-faster-for-large-datasets","title":"Export with Polars (Faster for Large Datasets)","text":"<pre><code># Get data as Polars DataFrame\nteams_pl = scrapeTeams(output_format=\"polars\")\n\n# Export to various formats\nteams_pl.write_csv('output/teams_polars.csv')\nteams_pl.write_json('output/teams_polars.json')\nteams_pl.write_parquet('output/teams_polars.parquet')\n\nprint(f\"\u2705 Exported {len(teams_pl)} teams using Polars to:\")\nprint(\"   - output/teams_polars.csv\")\nprint(\"   - output/teams_polars.json\")\nprint(\"   - output/teams_polars.parquet\")\n</code></pre>"},{"location":"examples/export/#export-to-sqlite-database","title":"Export to SQLite Database","text":"<pre><code>import sqlite3\n\n# Create database connection\nconn = sqlite3.connect('output/nhl_data.db')\n\n# Save multiple tables\nteams.to_sql('teams', conn, if_exists='replace', index=False)\nstandings.to_sql('standings', conn, if_exists='replace', index=False)\nschedule.to_sql('schedule', conn, if_exists='replace', index=False)\n\nprint(\"\u2705 Saved to SQLite database: output/nhl_data.db\")\nprint(f\"   Tables: teams ({len(teams)} rows), standings ({len(standings)} rows), schedule ({len(schedule)} rows)\")\n\n# Query the database\nquery = \"SELECT fullName, id, teamPlaceName FROM teams LIMIT 5\"\nresult = pd.read_sql(query, conn)\nprint(\"\\n   Sample query result:\")\nresult\n\nconn.close()\n</code></pre>"},{"location":"examples/export/#incremental-export-append-mode","title":"Incremental Export (Append Mode)","text":"<pre><code>output_file = 'output/incremental_games.csv'\n\n# Remove if exists (for demo)\nif os.path.exists(output_file):\n    os.remove(output_file)\n\n# Scrape games incrementally\nfor gid in game_ids[:2] if game_ids else []:\n    print(f\"Scraping game {gid}...\")\n    pbp = scrapePlays(gid)\n\n    # Append to CSV (create if doesn't exist)\n    if os.path.exists(output_file):\n        pbp.to_csv(output_file, mode='a', header=False, index=False)\n        print(f\"   Appended {len(pbp)} events\")\n    else:\n        pbp.to_csv(output_file, mode='w', header=True, index=False)\n        print(f\"   Created file with {len(pbp)} events\")\n\nif os.path.exists(output_file):\n    total_rows = len(pd.read_csv(output_file))\n    print(f\"\\n\u2705 Total events in incremental file: {total_rows}\")\n</code></pre>"},{"location":"examples/export/#export-selected-columns","title":"Export Selected Columns","text":"<pre><code>from scrapernhl.scrapers.roster import scrapeRoster\n\n# Scrape roster\nroster = scrapeRoster(\"MTL\", \"20252026\")\n\n# Export only specific columns\ncolumns_to_export = ['firstName.default', 'lastName.default', 'sweaterNumber', 'positionCode', 'heightInInches', 'weightInPounds']\nroster[columns_to_export].to_csv('output/mtl_roster_simple.csv', index=False)\nprint(f\"\u2705 Saved simplified roster ({len(columns_to_export)} columns) to output/mtl_roster_simple.csv\")\n</code></pre>"},{"location":"examples/export/#export-with-custom-formatting","title":"Export with Custom Formatting","text":"<pre><code># Add custom columns and formatting\nschedule_formatted = schedule.copy()\nschedule_formatted['gameDate'] = pd.to_datetime(schedule_formatted['gameDate']).dt.strftime('%Y-%m-%d')\nschedule_formatted['season'] = '2025-26'\nschedule_formatted['scraped_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n# Export with custom column order\ncolumn_order = ['gameDate', 'homeTeam.abbrev', 'awayTeam.abbrev', 'gameState', 'season', 'scraped_at']\nschedule_formatted[column_order].to_csv('output/mtl_schedule_formatted.csv', index=False)\nprint(\"\u2705 Saved formatted schedule to output/mtl_schedule_formatted.csv\")\nprint(\"   Custom columns: season, scraped_at\")\nprint(\"   Date format: YYYY-MM-DD\")\n</code></pre>"},{"location":"examples/export/#list-all-exported-files","title":"List All Exported Files","text":"<pre><code># List all files in output directory\noutput_files = os.listdir('output')\nprint(f\"\\n\ud83d\udcc1 Exported {len(output_files)} files to output/ directory:\\n\")\n\nfor file in sorted(output_files):\n    filepath = os.path.join('output', file)\n    size = os.path.getsize(filepath) / 1024\n    print(f\"   {file:&lt;30} {size:&gt;8.1f} KB\")\n</code></pre>"},{"location":"examples/export/#see-also","title":"See Also","text":"<ul> <li>Basic Scraping - Getting the data</li> <li>Advanced Examples - Data processing</li> <li>API Reference - Function documentation</li> </ul>"},{"location":"examples/scraping/","title":"Basic Scraping Examples","text":"<p>Examples showing how to scrape NHL data.</p>"},{"location":"examples/scraping/#setup","title":"Setup","text":"<pre><code>import pandas as pd\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n</code></pre>"},{"location":"examples/scraping/#1-scraping-nhl-teams","title":"1. Scraping NHL Teams","text":"<p>Retrieve information about all NHL teams including their names, IDs, and locations.</p> <pre><code>from scrapernhl.scrapers.teams import scrapeTeams\n\n# Get all NHL teams\nteams = scrapeTeams()\nprint(f\"Found {len(teams)} teams\")\n\n# Display selected columns\nteams[['name.default', 'abbrev','id', 'placeName.default', 'commonName.default']].head(10)\n</code></pre>"},{"location":"examples/scraping/#2-scraping-team-schedule","title":"2. Scraping Team Schedule","text":"<p>Get the complete schedule for a specific team and season, including game dates, opponents, scores, and game states.</p> <pre><code>from scrapernhl.scrapers.schedule import scrapeSchedule\n\n# Get Montreal Canadiens schedule for current season\nschedule = scrapeSchedule(\"MTL\", \"20252026\")\nprint(f\"MTL has {len(schedule)} games this season\")\n\n# Show first 5 games with key information\nschedule[['gameDate', 'gameType', 'homeTeam.abbrev', 'homeTeam.score',\n          'awayTeam.abbrev', 'awayTeam.score', 'gameOutcome.lastPeriodType', 'gameState']].head()\n</code></pre>"},{"location":"examples/scraping/#3-current-standings","title":"3. Current Standings","text":"<p>Fetch the league standings for a specific date, including wins, losses, points, and point percentage.</p> <pre><code>from scrapernhl.scrapers.standings import scrapeStandings\nfrom datetime import datetime\n\n# Get current standings\ntoday = datetime.now().strftime(\"%Y-%m-%d\")\nstandings = scrapeStandings(today)\n\nprint(f\"Standings as of {today}:\")\nstandings[['teamName.default', 'teamAbbrev.default', 'gamesPlayed', 'wins', 'losses', 'otLosses', 'points', 'pointPctg', 'date']].sort_values(by='pointPctg', ascending=False).head(10)\n</code></pre>"},{"location":"examples/scraping/#4-team-roster","title":"4. Team Roster","text":"<p>Get the complete roster for a team, including player names, positions, physical attributes, and biographical information.</p> <pre><code>from scrapernhl.scrapers.roster import scrapeRoster\n\n# Get Montreal Canadiens roster\nroster = scrapeRoster(\"MTL\", \"20252026\")\n\n# Separate by position\nforwards = roster[roster['positionCode'].isin(['C', 'L', 'R'])]  # Forwards: Centers, Left Wings, Right Wings\ndefensemen = roster[roster['positionCode'] == 'D']\ngoalies = roster[roster['positionCode'] == 'G']\n\nprint(f\"Forwards: {len(forwards)}, Defense: {len(defensemen)}, Goalies: {len(goalies)}\")\n\nprint(\"\\nForwards:\")\nforwards[['id', 'firstName.default', 'lastName.default', 'positionCode', 'shootsCatches', \n          'sweaterNumber', 'heightInInches', 'weightInPounds', 'birthDate', 'birthCountry']].assign(team=\"MTL\").head(10)\n</code></pre>"},{"location":"examples/scraping/#5-player-statistics","title":"5. Player Statistics","text":"<p>Scrape player statistics for both skaters and goalies, including goals, assists, points, wins, and save percentage.</p> <pre><code>from scrapernhl.scrapers.stats import scrapeTeamStats\n\n# Get skater stats\nskaters = scrapeTeamStats(\"MTL\", \"20252026\", session=2, goalies=False)\nprint(\"Top 10 scorers:\")\nskaters.nlargest(10, 'points')[['playerId', 'firstName.default', 'lastName.default', 'positionCode', \n                                'gamesPlayed', 'goals', 'assists', 'points']].assign(pointsPerGame=lambda df: df['points'].div(df['gamesPlayed']))\n\n# Get goalie stats\ngoalies = scrapeTeamStats(\"MTL\", \"20252026\", session=2, goalies=True)\nprint(\"\\nGoalie statistics:\")\ngoalies[['playerId', 'firstName.default', 'lastName.default', 'gamesPlayed', 'wins', 'losses',\n         'overtimeLosses', 'goalsAgainstAverage', 'savePercentage']]\n</code></pre>"},{"location":"examples/scraping/#6-play-by-play-data","title":"6. Play-by-Play Data","text":"<p>Retrieve detailed play-by-play data for a specific game, including all events like shots, goals, hits, and faceoffs.</p> <pre><code>from scrapernhl.scrapers.games import scrapePlays\n\n# Get a recent game ID from schedule\ncompleted_games = schedule[schedule['gameState'] == 'OFF']\nif len(completed_games) &gt; 0:\n    game_id = completed_games.iloc[0]['id']\n    print(f\"Scraping game {game_id}...\")\n\n    pbp = scrapePlays(game_id)\n    print(f\"Game has {len(pbp)} events\")\n\n    # Show event types\n    print(\"\\nEvent counts:\")\n    pbp['typeDescKey'].value_counts()\n\n    # Show first few events\n    print(\"\\nFirst 10 events:\")\n    pbp[['periodDescriptor.number', 'timeInPeriod', 'typeDescKey', 'details.eventOwnerTeamId', 'gameId']].head(10)\nelse:\n    print(\"No completed games found in schedule\")\n</code></pre>"},{"location":"examples/scraping/#7-draft-data","title":"7. Draft Data","text":"<p>Access historical NHL draft data including player information, draft position, and team selections.</p> <pre><code>from scrapernhl.scrapers.draft import scrapeDraftData\n\n# Get 2025 first round picks\ndraft_2025_r1 = scrapeDraftData(\"2025\", 1)\nprint(f\"2025 Draft - Round 1: {len(draft_2025_r1)} picks\")\ndraft_2025_r1[['round', 'pickInRound', 'overallPick', 'teamAbbrev', 'firstName.default', 'lastName.default',\n               'positionCode', 'countryCode', 'height', 'weight', 'year']].head(10)\n</code></pre>"},{"location":"examples/scraping/#8-using-polars-alternative-to-pandas","title":"8. Using Polars (Alternative to Pandas)","text":"<p>Polars is a faster alternative to Pandas for large datasets. The scraper supports both output formats.</p> <pre><code># Get data as Polars DataFrame (faster for large datasets)\nteams_pl = scrapeTeams(output_format=\"polars\")\nprint(f\"Type: {type(teams_pl)}\")\nprint(f\"Shape: {teams_pl.shape}\")\n\n# Polars syntax\nteams_pl.select(['name', 'abbrev','id', 'placeName', 'commonName']).head(5)\n</code></pre>"},{"location":"examples/scraping/#9-backward-compatibility-test","title":"9. Backward Compatibility Test","text":"<p>The package maintains backward compatibility with older import styles for ease of migration.</p> <pre><code># The old import style still works\nfrom scrapernhl import scrapeTeams, scrapeSchedule\n\nteams_old_style = scrapeTeams()\nprint(f\"Old import style works: {len(teams_old_style)} teams scraped\")\n</code></pre>"},{"location":"examples/scraping/#see-also","title":"See Also","text":"<ul> <li>Advanced Examples - Feature engineering, analytics</li> <li>Data Export - Saving data to files</li> <li>API Reference - Complete API documentation</li> </ul> <pre><code>from scrapernhl.scrapers.teams import scrapeTeams\n\n# Get all NHL teams\nteams = scrapeTeams()\nprint(f\"Found {len(teams)} teams\")\n\n# Display selected columns\nteams[['name.default', 'abbrev','id', 'placeName.default', 'commonName.default']].head(10)\n</code></pre>"},{"location":"examples/scraping/#scraping-schedule","title":"Scraping Schedule","text":"<pre><code>from scrapernhl.scrapers.schedule import scrapeSchedule\n\n# Get Montreal Canadiens schedule for current season\nschedule = scrapeSchedule(\"MTL\", \"20252026\")\nprint(f\"MTL has {len(schedule)} games this season\")\n\n# Show first 5 games with key information\nschedule[['gameDate', 'gameType', 'homeTeam.abbrev', 'homeTeam.score',\n          'awayTeam.abbrev', 'awayTeam.score', 'gameOutcome.lastPeriodType', 'gameState']].head()\n</code></pre>"},{"location":"examples/scraping/#scraping-standings","title":"Scraping Standings","text":"<pre><code>from scrapernhl.scrapers.standings import scrapeStandings\nfrom datetime import datetime\n\n# Get current standings\ntoday = datetime.now().strftime(\"%Y-%m-%d\")\nstandings = scrapeStandings(today)\n\nprint(f\"Standings as of {today}:\")\nstandings[['teamName.default', 'teamAbbrev.default', 'gamesPlayed', 'wins', 'losses', 'otLosses', 'points', 'pointPctg', 'date']].sort_values(by='pointPctg', ascending=False).head(10)\n</code></pre>"},{"location":"examples/scraping/#getting-play-by-play-data","title":"Getting Play-by-Play Data","text":"<pre><code>from scrapernhl.scrapers.games import scrapePlays\n\n# Get play-by-play for a specific game\ngame_id = 2024020001\npbp = scrapePlays(game_id)\n\nprint(f\"Game {game_id} has {len(pbp)} events\")\n\n# Show event types\nprint(\"\\nEvent counts:\")\npbp['typeDescKey'].value_counts()\n\n# Show first few events\nprint(\"\\nFirst 10 events:\")\npbp[['periodDescriptor.number', 'timeInPeriod', 'typeDescKey', 'details.eventOwnerTeamId', 'gameId']].head(10)\n</code></pre>"},{"location":"examples/scraping/#with-goal-replay-data","title":"With Goal Replay Data","text":"<pre><code>from scrapernhl.scrapers.games import scrapePlays\n\n# Include goal replay data\npbp = scrapePlays(2024020001, addGoalReplayData=True)\n\n# Filter for goals only\ngoals = pbp[pbp['eventType'] == 'goal']\nprint(f\"Goals scored: {len(goals)}\")\n</code></pre>"},{"location":"examples/scraping/#scraping-multiple-games","title":"Scraping Multiple Games","text":"<pre><code>from scrapernhl.scrapers.games import scrapePlays\nimport pandas as pd\n\n# Scrape multiple games\ngame_ids = [2024020001, 2024020002, 2024020003]\nall_plays = []\n\nfor game_id in game_ids:\n    print(f\"Scraping game {game_id}...\")\n    pbp = scrapePlays(game_id)\n    all_plays.append(pbp)\n\n# Combine all games\ncombined_pbp = pd.concat(all_plays, ignore_index=True)\nprint(f\"Total events across {len(game_ids)} games: {len(combined_pbp)}\")\n</code></pre>"},{"location":"examples/scraping/#getting-roster-information","title":"Getting Roster Information","text":"<pre><code>from scrapernhl.scrapers.roster import scrapeRoster\n\n# Get Montreal Canadiens roster\nroster = scrapeRoster(\"MTL\", \"20252026\")\n\n# Separate by position\nforwards = roster[roster['positionCode'].isin(['C', 'L', 'R'])]  # Forwards: Centers, Left Wings, Right Wings\ndefensemen = roster[roster['positionCode'] == 'D']\ngoalies = roster[roster['positionCode'] == 'G']\n\nprint(f\"Forwards: {len(forwards)}, Defense: {len(defensemen)}, Goalies: {len(goalies)}\")\n\nprint(\"\\nForwards:\")\nforwards[['id', 'firstName.default', 'lastName.default', 'positionCode', 'shootsCatches', \n          'sweaterNumber', 'heightInInches', 'weightInPounds', 'birthDate', 'birthCountry']].assign(team=\"MTL\").head(10)\n</code></pre>"},{"location":"examples/scraping/#getting-player-statistics","title":"Getting Player Statistics","text":"<pre><code>from scrapernhl.scrapers.stats import scrapeTeamStats\n\n# Get skater stats\nskaters = scrapeTeamStats(\"MTL\", \"20252026\", session=2, goalies=False)\nprint(\"Top 10 scorers:\")\nskaters.nlargest(10, 'points')[['playerId', 'firstName.default', 'lastName.default', 'positionCode', \n                                'gamesPlayed', 'goals', 'assists', 'points']].assign(pointsPerGame=lambda df: df['points'].div(df['gamesPlayed']))\n\n# Get goalie stats\ngoalies = scrapeTeamStats(\"MTL\", \"20252026\", session=2, goalies=True)\nprint(\"\\nGoalie statistics:\")\ngoalies[['playerId', 'firstName.default', 'lastName.default', 'gamesPlayed', 'wins', 'losses',\n         'overtimeLosses', 'goalsAgainstAverage', 'savePercentage']]\n</code></pre>"},{"location":"examples/scraping/#getting-draft-data","title":"Getting Draft Data","text":"<pre><code>from scrapernhl.scrapers.draft import scrapeDraftData\n\n# Get 2025 first round picks\ndraft_2025_r1 = scrapeDraftData(\"2025\", 1)\nprint(f\"2025 Draft - Round 1: {len(draft_2025_r1)} picks\")\ndraft_2025_r1[['round', 'pickInRound', 'overallPick', 'teamAbbrev', 'firstName.default', 'lastName.default',\n               'positionCode', 'countryCode', 'height', 'weight', 'year']].head(10)\n</code></pre>"},{"location":"examples/scraping/#using-polars-instead-of-pandas","title":"Using Polars Instead of Pandas","text":"<pre><code># Get data as Polars DataFrame (faster for large datasets)\nteams_pl = scrapeTeams(output_format=\"polars\")\nprint(f\"Type: {type(teams_pl)}\")\nprint(f\"Shape: {teams_pl.shape}\")\n\n# Polars syntax\nteams_pl.select(['name', 'abbrev','id', 'placeName', 'commonName']).head(5)\n</code></pre>"},{"location":"examples/scraping/#backward-compatible-style","title":"Backward Compatible Style","text":"<p>If you have existing code, the old import style still works:</p> <pre><code># The old import style still works\nfrom scrapernhl import scrapeTeams, scrapeSchedule\n\nteams_old_style = scrapeTeams()\nprint(f\"Old import style works: {len(teams_old_style)} teams scraped\")\n</code></pre>"},{"location":"examples/scraping/#error-handling","title":"Error Handling","text":"<pre><code>from scrapernhl.scrapers.games import scrapePlays\n\ntry:\n    pbp = scrapePlays(9999999999)  # Invalid game ID\nexcept Exception as e:\n    print(f\"Error scraping game: {e}\")\n</code></pre>"},{"location":"examples/scraping/#async-scraping-advanced","title":"Async Scraping (Advanced)","text":"<p>For scraping multiple games efficiently:</p> <pre><code>import asyncio\nfrom scrapernhl import scrape_game_async\n\nasync def scrape_multiple_games(game_ids):\n    tasks = [scrape_game_async(game_id) for game_id in game_ids]\n    results = await asyncio.gather(*tasks)\n    return results\n\n# Run async scraping\ngame_ids = [2024020001, 2024020002, 2024020003]\nresults = asyncio.run(scrape_multiple_games(game_ids))\nprint(f\"Scraped {len(results)} games\")\n</code></pre>"},{"location":"examples/scraping/#see-also_1","title":"See Also","text":"<ul> <li>Advanced Examples - Feature engineering, analytics</li> <li>Data Export - Saving data to files</li> <li>API Reference - Complete API documentation</li> </ul>"}]}