{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab753805",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a640b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('output', exist_ok=True)\n",
    "print(\"Created output/ directory for exported files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f710c",
   "metadata": {},
   "source": [
    "## 1. Export to CSV\n",
    "\n",
    "CSV (Comma-Separated Values) is a simple, widely-supported format ideal for quick data exports and Excel compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b5ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapernhl.scrapers.teams import scrapeTeams\n",
    "from scrapernhl.scrapers.schedule import scrapeSchedule\n",
    "\n",
    "# Scrape teams\n",
    "teams = scrapeTeams()\n",
    "teams.to_csv('output/nhl_teams.csv', index=False)\n",
    "print(f\"‚úÖ Saved {len(teams)} teams to output/nhl_teams.csv\")\n",
    "\n",
    "# Scrape schedule\n",
    "schedule = scrapeSchedule(\"MTL\", \"20252026\")\n",
    "schedule.to_csv('output/mtl_schedule.csv', index=False)\n",
    "print(f\"‚úÖ Saved {len(schedule)} games to output/mtl_schedule.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138435c3",
   "metadata": {},
   "source": [
    "## 2. Export to Excel (Multiple Sheets)\n",
    "\n",
    "Excel format allows multiple sheets in one file, perfect for organizing related datasets together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c99382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapernhl.scrapers.standings import scrapeStandings\n",
    "from scrapernhl.scrapers.stats import scrapeTeamStats\n",
    "\n",
    "# Scrape data\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "standings = scrapeStandings(today)\n",
    "skaters = scrapeTeamStats(\"MTL\", \"20252026\", goalies=False)\n",
    "goalies = scrapeTeamStats(\"MTL\", \"20252026\", goalies=True)\n",
    "\n",
    "# Save to Excel with multiple sheets\n",
    "with pd.ExcelWriter('output/nhl_data.xlsx', engine='openpyxl') as writer:\n",
    "    standings.to_excel(writer, sheet_name='Standings', index=False)\n",
    "    skaters.to_excel(writer, sheet_name='Skaters', index=False)\n",
    "    goalies.to_excel(writer, sheet_name='Goalies', index=False)\n",
    "\n",
    "print(f\"‚úÖ Saved Excel file with 3 sheets to output/nhl_data.xlsx\")\n",
    "print(f\"   - Standings: {len(standings)} teams\")\n",
    "print(f\"   - Skaters: {len(skaters)} players\")\n",
    "print(f\"   - Goalies: {len(goalies)} goalies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb82113",
   "metadata": {},
   "source": [
    "## 3. Export to JSON\n",
    "\n",
    "JSON is ideal for web applications and APIs. Use `indent=2` for readable format or `lines=True` for efficient large file handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37167b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapernhl.scrapers.games import scrapePlays\n",
    "\n",
    "# Get a completed game\n",
    "completed_games = schedule[schedule['gameState'] == 'OFF']\n",
    "if len(completed_games) > 0:\n",
    "    game_id = completed_games.iloc[0]['id']\n",
    "    \n",
    "    # Scrape play-by-play\n",
    "    pbp = scrapePlays(game_id)\n",
    "    \n",
    "    # Save as JSON (pretty printed)\n",
    "    pbp.to_json('output/game_pbp.json', orient='records', indent=2)\n",
    "    print(f\"‚úÖ Saved {len(pbp)} events to output/game_pbp.json (pretty format)\")\n",
    "    \n",
    "    # Save as JSON lines (more efficient for large files)\n",
    "    pbp.to_json('output/game_pbp.jsonl', orient='records', lines=True)\n",
    "    print(f\"‚úÖ Saved {len(pbp)} events to output/game_pbp.jsonl (lines format)\")\n",
    "    \n",
    "    # Compare file sizes\n",
    "    json_size = os.path.getsize('output/game_pbp.json') / 1024\n",
    "    jsonl_size = os.path.getsize('output/game_pbp.jsonl') / 1024\n",
    "    print(f\"   File sizes: JSON={json_size:.1f}KB, JSONL={jsonl_size:.1f}KB\")\n",
    "else:\n",
    "    print(\"No completed games found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a42a4b",
   "metadata": {},
   "source": [
    "## 4. Export to Parquet (Recommended for Large Datasets)\n",
    "\n",
    "Parquet is a columnar storage format that offers excellent compression and fast read/write performance for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f43ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape multiple games\n",
    "game_ids = completed_games.head(3)['id'].tolist() if len(completed_games) >= 3 else []\n",
    "\n",
    "if game_ids:\n",
    "    all_pbp = []\n",
    "    for gid in game_ids:\n",
    "        print(f\"Scraping game {gid}...\")\n",
    "        pbp = scrapePlays(gid)\n",
    "        all_pbp.append(pbp)\n",
    "    \n",
    "    # Combine\n",
    "    combined = pd.concat(all_pbp, ignore_index=True)\n",
    "    \n",
    "    # Save as Parquet (compressed)\n",
    "    combined.to_parquet('output/games_pbp.parquet', index=False, compression='snappy')\n",
    "    print(f\"‚úÖ Saved {len(combined)} events to output/games_pbp.parquet\")\n",
    "    \n",
    "    # Read it back to verify\n",
    "    df = pd.read_parquet('output/games_pbp.parquet')\n",
    "    print(f\"‚úÖ Verified: Loaded {len(df)} events from parquet\")\n",
    "    \n",
    "    # Compare with CSV\n",
    "    combined.to_csv('output/games_pbp.csv', index=False)\n",
    "    parquet_size = os.path.getsize('output/games_pbp.parquet') / 1024\n",
    "    csv_size = os.path.getsize('output/games_pbp.csv') / 1024\n",
    "    print(f\"   File sizes: Parquet={parquet_size:.1f}KB, CSV={csv_size:.1f}KB\")\n",
    "    print(f\"   Compression: {100 * (1 - parquet_size/csv_size):.1f}% smaller\")\n",
    "else:\n",
    "    print(\"No completed games available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0214de4d",
   "metadata": {},
   "source": [
    "## 5. Export with Polars (Faster)\n",
    "\n",
    "Polars provides faster I/O operations than Pandas, especially beneficial for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2fb671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data as Polars DataFrame\n",
    "teams_pl = scrapeTeams(output_format=\"polars\")\n",
    "\n",
    "# Export to various formats\n",
    "teams_pl.write_csv('output/teams_polars.csv')\n",
    "teams_pl.write_json('output/teams_polars.json')\n",
    "teams_pl.write_parquet('output/teams_polars.parquet')\n",
    "\n",
    "print(f\"‚úÖ Exported {len(teams_pl)} teams using Polars to:\")\n",
    "print(\"   - output/teams_polars.csv\")\n",
    "print(\"   - output/teams_polars.json\")\n",
    "print(\"   - output/teams_polars.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec390a13",
   "metadata": {},
   "source": [
    "## 6. Export to SQLite Database\n",
    "\n",
    "SQLite allows you to store multiple tables in a single database file and query them with SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739ea475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create database connection\n",
    "conn = sqlite3.connect('output/nhl_data.db')\n",
    "\n",
    "# Save multiple tables\n",
    "teams.to_sql('teams', conn, if_exists='replace', index=False)\n",
    "standings.to_sql('standings', conn, if_exists='replace', index=False)\n",
    "schedule.to_sql('schedule', conn, if_exists='replace', index=False)\n",
    "\n",
    "print(\"‚úÖ Saved to SQLite database: output/nhl_data.db\")\n",
    "print(f\"   Tables: teams ({len(teams)} rows), standings ({len(standings)} rows), schedule ({len(schedule)} rows)\")\n",
    "\n",
    "# Query the database\n",
    "query = \"SELECT fullName, id, teamPlaceName FROM teams LIMIT 5\"\n",
    "result = pd.read_sql(query, conn)\n",
    "print(\"\\n   Sample query result:\")\n",
    "display(result)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3603d42a",
   "metadata": {},
   "source": [
    "## 7. Incremental Export (Append Mode)\n",
    "\n",
    "Useful for processing large datasets in batches or adding new data to existing files without rewriting everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094c4d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'output/incremental_games.csv'\n",
    "\n",
    "# Remove if exists (for demo)\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "# Scrape games incrementally\n",
    "for gid in game_ids[:2] if game_ids else []:\n",
    "    print(f\"Scraping game {gid}...\")\n",
    "    pbp = scrapePlays(gid)\n",
    "    \n",
    "    # Append to CSV (create if doesn't exist)\n",
    "    if os.path.exists(output_file):\n",
    "        pbp.to_csv(output_file, mode='a', header=False, index=False)\n",
    "        print(f\"   Appended {len(pbp)} events\")\n",
    "    else:\n",
    "        pbp.to_csv(output_file, mode='w', header=True, index=False)\n",
    "        print(f\"   Created file with {len(pbp)} events\")\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    total_rows = len(pd.read_csv(output_file))\n",
    "    print(f\"\\n‚úÖ Total events in incremental file: {total_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2875f",
   "metadata": {},
   "source": [
    "## 8. Export Selected Columns\n",
    "\n",
    "Export only the columns you need to reduce file size and improve readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapernhl.scrapers.roster import scrapeRoster\n",
    "\n",
    "# Scrape roster\n",
    "roster = scrapeRoster(\"MTL\", \"20252026\")\n",
    "\n",
    "# Export only specific columns\n",
    "columns_to_export = ['firstName.default', 'lastName.default', 'sweaterNumber', 'positionCode', 'heightInInches', 'weightInPounds']\n",
    "roster[columns_to_export].to_csv('output/mtl_roster_simple.csv', index=False)\n",
    "print(f\"‚úÖ Saved simplified roster ({len(columns_to_export)} columns) to output/mtl_roster_simple.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb469f75",
   "metadata": {},
   "source": [
    "## 9. Export with Custom Formatting\n",
    "\n",
    "Add custom columns, format dates, and reorder columns before exporting for better data organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322819a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom columns and formatting\n",
    "schedule_formatted = schedule.copy()\n",
    "schedule_formatted['gameDate'] = pd.to_datetime(schedule_formatted['gameDate']).dt.strftime('%Y-%m-%d')\n",
    "schedule_formatted['season'] = '2025-26'\n",
    "schedule_formatted['scraped_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Export with custom column order\n",
    "column_order = ['gameDate', 'homeTeam.abbrev', 'awayTeam.abbrev', 'gameState', 'season', 'scraped_at']\n",
    "schedule_formatted[column_order].to_csv('output/mtl_schedule_formatted.csv', index=False)\n",
    "print(\"‚úÖ Saved formatted schedule to output/mtl_schedule_formatted.csv\")\n",
    "print(\"   Custom columns: season, scraped_at\")\n",
    "print(\"   Date format: YYYY-MM-DD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b42a13f",
   "metadata": {},
   "source": [
    "## 10. List All Exported Files\n",
    "\n",
    "Review all files created during this session with their sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a00d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in output directory\n",
    "output_files = os.listdir('output')\n",
    "print(f\"\\nüìÅ Exported {len(output_files)} files to output/ directory:\\n\")\n",
    "\n",
    "for file in sorted(output_files):\n",
    "    filepath = os.path.join('output', file)\n",
    "    size = os.path.getsize(filepath) / 1024\n",
    "    print(f\"   {file:<30} {size:>8.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40efbd6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- ‚úÖ Export to CSV (simple and efficient)\n",
    "- ‚úÖ Export to Excel with multiple sheets\n",
    "- ‚úÖ Export to JSON (pretty and lines format)\n",
    "- ‚úÖ Export to Parquet (compressed, best for large data)\n",
    "- ‚úÖ Export with Polars (faster processing)\n",
    "- ‚úÖ Export to SQLite database (queryable)\n",
    "- ‚úÖ Incremental/append mode export\n",
    "- ‚úÖ Export selected columns\n",
    "- ‚úÖ Export with custom formatting\n",
    "\n",
    "All export methods working! üì¶‚úÖ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
